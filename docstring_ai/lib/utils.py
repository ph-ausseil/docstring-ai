# Docstring generated by docstring-ai : http://github.com/ph-ausseil/docstring-ai
"""
This module provides various utilities for managing and processing Python files and Git repositories. It includes functionality for ensuring docstring headers, checking repository statuses, caching file states, and generating diffs between file versions.

Imports:
    os: For file path operations.
    openai: OpenAI API functionalities.
    argparse: For argument parsing.
    time: For handling time-related functionalities.
    json: For JSON operations.
    chromadb: For accessing ChromaDB functionalities.
    logging: For logging events and errors.
    Settings: For ChromaDB configuration settings.
    embedding_functions: For using OpenAI embedding functions.
    tiktoken: For token counting.
    List, Dict: For type hinting.
    hashlib: For hashing file content.
    load_dotenv: For loading environment variables from a .env file.
    datetime: For handling date and time.
    Github, GithubException: For GitHub API functionalities.
    subprocess: For running shell commands.
    sys: For system-specific parameters and functions.
    difflib: For generating diffs between file contents.
    DOCSTRING_AI_TAG, DATA_PATH: For configuration constants.
"""
from tqdm import tqdm
import os
import openai
import argparse
import time
import json
import chromadb
import logging
from chromadb.config import Settings
from chromadb.utils import embedding_functions
import tiktoken
from typing import List, Dict, Optional
import hashlib
from dotenv import load_dotenv
from datetime import datetime
from github import Github, GithubException
import subprocess
import sys
from pathlib import Path
import difflib
from docstring_ai.lib.config import DOCSTRING_AI_TAG, DATA_PATH
from pydantic import BaseModel, Field


def get_parent_folders(file_path: Path, base_path: Path) -> list:
    """
    Given a file path and a base path, return a list of relative paths of parent folders.

    Args:
        file_path (Path): The path of the file.
        base_path (Path): The base path to calculate the relative paths.

    Returns:
        list: A list of relative parent folder paths from the base to the file's immediate parent.
    
    Raises:
        ValueError: If the base path is not a parent of the file path.
    """
    # Ensure both paths are absolute
    file_path = Path(file_path).resolve()
    base_path = Path(base_path).resolve()

    # Validate that the base path is a parent of the file path
    if not base_path in file_path.parents and base_path != file_path:
        raise ValueError("The base path must be a parent of the file path.")

    # Compute the relative path to the file from the base path
    relative_path = file_path.relative_to(base_path)

    # Collect the relative parent paths
    parents = []
    current_path = Path()
    for part in relative_path.parts[:-1]:  # Skip the file name
        current_path /= part
        parents.append(str(current_path))

    return parents


def filter_files_by_hash(file_paths: List[str], repo_path: str, cache: Dict[str, str]) -> List[str]:
    """
    Filters files based on SHA-256 hash and cache.

    Args:
        file_paths (List[str]): List of file paths to filter.
        repo_path (str): Path to the repository.
        cache (Dict[str, str]): Cache dictionary storing file hashes.

    Returns:
        List[str]: List of file paths that need processing.
    """
    changed_files = []
    logging.debug("Starting file hash verification...")
    
    with tqdm(total=len(file_paths), desc="Verifying file hashes", unit="file") as pbar:
        for file_path in file_paths:
            try:
                current_hash = compute_sha256(file_path)
                relative_path = os.path.relpath(file_path, repo_path)
                cached_hash = cache.get(relative_path)
                
                if current_hash != cached_hash:
                    changed_files.append(file_path)
            except Exception as e:
                logging.error(f"Error verifying hash for {file_path}: {e}")
            finally:
                pbar.update(1)
    
    logging.debug(f"Hash verification completed. {len(changed_files)} files require processing.")
    return changed_files



def ensure_docstring_header(content: str) -> str:
    """
    Ensures the content contains the docstring header. If not, prepends the header.

    Args:
        content (str): The content to be checked and potentially modified.

    Returns:
        str: The updated content with the docstring header ensured.
    """
    if DOCSTRING_AI_TAG not in content:
        return DOCSTRING_AI_TAG + "\n" + content
    return content


def file_has_uncommitted_changes(repo_path: str, file_path: str) -> bool:
    """
    Checks if the specific file has uncommitted changes in the Git repository.
    Returns True if there are uncommitted changes for the file, False otherwise.
    
    Args:
        repo_path (str): The path to the repository.
        file_path (str): The path to the specific file.

    Returns:
        bool: True if there are uncommitted changes for the file, False otherwise.
    """
    try:
        result = subprocess.run(
            ["git", "-C", repo_path, "diff", "--name-only"],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            check=True
        )
        changed_files = result.stdout.strip().split('\n')
        return os.path.relpath(file_path, repo_path) in changed_files
    except subprocess.CalledProcessError as e:
        logging.error(f"Git command failed while checking uncommitted changes for {file_path}: {e}")
        return False


def prompt_user_confirmation(message: str) -> bool:
    """
    Prompts the user for a yes/no confirmation.
    
    Args:
        message (str): The message to display for confirmation prompt.

    Returns:
        bool: True if the user confirms, False otherwise.
    """
    while True:
        response = input(f"{message} (yes/no): ").strip().lower()
        if response in ['yes', 'y']:
            return True
        elif response in ['no', 'n']:
            return False
        else:
            print("Please respond with 'yes' or 'no'.")


def check_git_repo(repo_path: str) -> bool:
    """
    Checks if the specified directory is a Git repository.
    
    Args:
        repo_path (str): The path to the repository to check.

    Returns:
        bool: True if the directory is a Git repository, False otherwise.
    """
    try:
        subprocess.run(
            ["git", "-C", repo_path, "rev-parse", "--is-inside-work-tree"],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            check=True
        )
        logging.debug("✅ Git repository detected.")
        return True
    except subprocess.CalledProcessError:
        logging.warning("❌ The specified path is not a Git repository.")
        return False
    except FileNotFoundError:
        logging.warning("❌ Git is not installed or not available in the PATH.")
        return False
    except Exception as e:
        logging.error(f"❌ Error checking Git repository: {e}")
        return False


def repo_has_uncommitted_changes(repo_path: str) -> bool:
    """
    Checks for uncommitted changes in the Git repository.
    
    Args:
        repo_path (str): The path to the Git repository.

    Returns:
        bool: True if there are uncommitted changes, False otherwise.
    """
    try:
        result = subprocess.run(
            ["git", "-C", repo_path, "status", "--porcelain"],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            check=True
        )
        if result.stdout.strip():
            logging.warning("⚠️ Uncommitted changes detected in the repository!")
            print("Consider committing or stashing your changes before running the script.")
            confirm = input("Do you wish to continue? (yes/no): ").strip().lower()
            if confirm != "yes":
                print("Operation aborted by the user.")
                sys.exit(0)
            return True
        else:
            logging.debug("✅ No uncommitted changes detected.")
            return False
    except subprocess.CalledProcessError as e:
        logging.error(f"Error checking uncommitted changes: {e}")
        return False


def load_cache(cache_file: str) -> Dict[str, str]:
    """
    Loads the cache from a JSON file and returns a mapping of file paths to their SHA-256 hashes.
    
    Args:
        cache_file (str): The path to the cache JSON file.

    Returns:
        Dict[str, str]: A dictionary mapping file paths to their SHA-256 hashes. If the file does not exist, an empty dict is returned.
    """
    if not os.path.exists(cache_file):
        logging.debug(f"No cache file found at '{cache_file}'. Starting with an empty cache.")
        return {}
    try:
        with open(cache_file, 'r', encoding='utf-8') as f:
            cache = json.load(f)
        logging.debug(f"Loaded cache with {len(cache)} entries.")
        return cache
    except Exception as e:
        logging.error(f"Error loading cache file '{cache_file}': {e}")
        return {}


def save_cache(cache_file: str, cache: Dict[str, str]) -> None:
    """
    Saves the cache to a JSON file.
    
    Args:
        cache_file (str): The path to the cache JSON file.
        cache (Dict[str, str]): The cache data to be saved.
    """
    try:
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(cache, f, indent=2)
        logging.debug(f"Cache saved with {len(cache)} entries.")
    except Exception as e:
        logging.error(f"Error saving cache file '{cache_file}': {e}")


def get_python_files(repo_path: str) -> List[str]:
    """
    Retrieves a list of all Python files in the given repository.
    
    Args:
        repo_path (str): The local path to the GitHub repository.
    
    Returns:
        List[str]: A list of Python file paths.
    """
    python_files = []
    for root, dirs, files in os.walk(repo_path):
        # Skip hidden directories
        dirs[:] = [d for d in dirs if not d.startswith('.') or not "__pycache__"]  
        for file in files:
            if file.endswith('.py'):
                full_path = os.path.join(root, file)
                python_files.append(os.path.relpath(full_path, repo_path))
    logging.debug(f"Total Python files found: {len(python_files)}")
    return python_files


def sort_files_by_size(file_paths: List[str]) -> List[str]:
    """
    Sorts files in ascending order based on their file size.
    
    Args:
        file_paths (List[str]): A list of file paths to sort.

    Returns:
        List[str]: A list of sorted file paths.
    """
    sorted_files = sorted(file_paths, key=lambda x: os.path.getsize(x))
    logging.debug("Files sorted by size (ascending).")
    return sorted_files


def compute_sha256(file_path: str) -> str:
    """
    Computes the SHA-256 hash of a file.
    
    Args:
        file_path (str): The path to the file to compute the hash for.

    Returns:
        str: The SHA-256 hash of the file as a hexadecimal string.
    """
    sha256_hash = hashlib.sha256()
    try:
        with open(file_path, "rb") as f:
            # Read and update hash string value in blocks of 4K
            for byte_block in iter(lambda: f.read(4096), b""):
                sha256_hash.update(byte_block)
        return sha256_hash.hexdigest()
    except Exception as e:
        logging.error(f"Error computing SHA-256 for {file_path}: {e}")
        return ""


def traverse_repo(repo_path: str, pr_depth: int) -> Dict[int, List[str]]:
    """
    Traverse the repository and categorize folders based on their depth.
    
    Args:
        repo_path (str): The path to the Git repository.
        pr_depth (int): The maximum depth of folders to traverse.
    
    Returns:
        Dict[int, List[str]]: A dictionary mapping depth levels to lists of folder paths.
    """
    folder_dict = {}
    for root, dirs, files in os.walk(repo_path):
        # Calculate the current folder's depth relative to repo_path
        rel_path = os.path.relpath(root, repo_path)
        if rel_path == ".":
            depth = 0
        else:
            depth = rel_path.count(os.sep) + 1  # +1 because rel_path doesn't start with sep
        if depth <= pr_depth:
            folder_dict.setdefault(depth, []).append(root)
    return folder_dict


def create_backup(file_path: str) -> None:
    """
    Creates a backup of the given file with a timestamp to prevent overwriting existing backups.
    
    Args:
        file_path (str): The path of the file to back up.
    """
    timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
    backup_path = f"{file_path}.{timestamp}.bak"
    try:
        with open(file_path, 'r', encoding='utf-8') as original_file, \
             open(backup_path, 'w', encoding='utf-8') as backup_file:
            backup_file.write(original_file.read())
        logging.debug(f"Backup created at {backup_path}")
    except Exception as e:
        logging.error(f"Error creating backup for {file_path}: {e}")


def show_diff(original_code: str, modified_code: str) -> str:
    """
    Generates a unified diff between the original and modified code.
    
    Args:
        original_code (str): The original code as a string.
        modified_code (str): The modified code as a string.

    Returns:
        str: The unified diff as a string.
    """
    original_lines = original_code.splitlines(keepends=True)
    modified_lines = modified_code.splitlines(keepends=True)
    diff = difflib.unified_diff(original_lines, modified_lines, fromfile='original', tofile='modified')
    return ''.join(diff)
