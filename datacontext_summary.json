[
  {
    "file": "./docstring_ai/__init__.py",
    "description": "Operation failed due to an API error."
  },
  {
    "file": "./docstring_ai/lib/config.py",
    "description": "Here\u2019s a comprehensive and detailed description of the provided Python file:\n\n### Overview\n\nThis Python file defines constants and logging configuration for a machine learning-based application that interacts with models and APIs. The main functionalities include specifying model parameters, handling API interactions with retry logic, organizing data storage paths, and configuring a customized logging system with colored output for better readability. The module primarily aims at improving the maintainability and usability of code related to model inference and logging.\n\n### Main Functionalities\n\n1. **Model and API Configuration**: The module manages constants related to model configurations, such as model names, token limits, retry counts, and backoff strategies for API requests.\n\n2. **Context Storage Management**: It defines constants for file paths related to data caching and context management, which optimize the performance of data retrieval operations.\n\n3. **Custom Logging Configuration**: The module implements a custom logging setup that includes colored formatting for different log levels and filtering for specific libraries to enhance the clarity and relevance of log output.\n\n### Purpose\n\nThe purpose of this Python file can be summarized as follows:\n- To provide a structured approach for constants that configure model interactions and API handling.\n- To establish a comprehensive logging solution that improves the debuggability and traceability of code execution.\n- To optimize data management and caching strategies for efficient storage and retrieval of important context-related information.\n\n### Constants and Their Descriptions\n\nThe file includes a set of constants, each playing a significant role:\n\n1. **MODEL (str)**: \n   - Defines the specific model version to be used for processing tasks. It allows flexibility in updating or changing models as required.\n\n2. **MAX_TOKENS (int)**: \n   - Establishes the maximum number of tokens allowed in a single API request. This safeguard prevents errors relating to input size, ensuring it aligns with model processing capabilities.\n\n3. **EMBEDDING_MODEL (str)**: \n   - Specifies the name of the embedding model for converting text into numerical vectors. This is essential for tasks that require embedding representations.\n\n4. **MAX_RETRIES (int)**: \n   - Indicates the maximum number of retry attempts for API requests in case of temporary failures. This is useful for handling transient errors and improving application resilience.\n\n5. **RETRY_BACKOFF (int)**: \n   - Sets the time to wait before retrying after a failed API request, allowing for controlled retries and reducing server strain.\n\n6. **CHROMA_COLLECTION_NAME (str)**: \n   - Specifies the name of the collection within ChromaDB for storing context data, facilitating organized data retrieval throughout the application lifecycle.\n\n7. **DATA_PATH (Path)**: \n   - Defines the base directory for storing data-related files, using `Path` from the `pathlib` module for better path handling.\n\n8. **CACHE_FILE_NAME (str)**: \n   - Indicates the filename used for caching purposes, contributing to performance optimization by avoiding redundant computations.\n\n9. **CONTEXT_SUMMARY_PATH (str)**: \n   - Specifies the file path for storing context summaries, which provides a means for accessing context information across different processing tasks.\n\n10. **DOCSTRING_AI_TAG**: \n   - A constant indicating the origin of the docstring that was generated by a specific tool.\n\n### Classes\n\nThe module defines the following classes:\n\n1. **ColoredFormatter(logging.Formatter)**:\n   - A custom logging formatter that adds color coding to log messages based on their severity levels. The color mapping is defined within the class, enhancing the visibility and differentiation of log levels when outputted.\n\n   **Method**: \n   - `format(self, record)`: Overrides the default format method to apply color to the log level names.\n\n2. **ExcludeLibrariesFilter(logging.Filter)**:\n   - A logging filter that excludes log messages from specified libraries. This prevents clutter in logs from libraries that may generate excessive or non-critical logging output.\n\n   **Method**: \n   - `filter(self, record)`: Returns `True` if the log record's name does not start with any specified modules in the exclusion list.\n\n3. **HTTPRequestFilter(logging.Filter)**:\n   - Another logging filter specifically designed to exclude HTTP request logs, providing a cleaner output by not including extensive HTTP interaction logs.\n\n   **Method**: \n   - `filter(self, record)`: Checks if the message contains 'HTTP Request:' and returns `False` if it does, effectively filtering it out.\n\n### Functions\n\n1. **setup_logging()**:\n   - Configures the logging for the application by initializing handlers, formatters, and levels. It creates an instance of `ColoredFormatter` and sets it to a stream handler that outputs to the console. Logging level is set to DEBUG, which captures all levels of logs, and applies filters to suppress logs from specific libraries.\n\n### Structure and Intent\n\nThe structure of the module is straightforward, with a clear separation between configuration constants, custom logging classes, and utility functions. It imports necessary libraries like `Path` from `pathlib` for filesystem path handling and `logging` alongside `colorama` for enhanced logging user experience.\n\nThe intent is to provide clarity and ease-of-use for developers interacting with the model and debugging through logging. By employing object-oriented principles in the logging configuration and defining constants in a structured manner, the module fosters better coding practices and effective application management.\n\n### Conclusion\n\nIn summary, this Python module is integral for efficiently managing configurations related to model interactions, API handling, and logging. By centralizing constants and providing a robust logging setup, it enhances the maintainability, usability, and overall performance of a machine learning application. The well-defined logging structure also aids in debugging by ensuring clear visibility into the application's operational flow."
  },
  {
    "file": "./docstring_ai/lib/chroma_utils.py",
    "description": "Here\u2019s a comprehensive and detailed description of the provided Python file:\n\n### Overview\n\nThis Python file is designed to facilitate the embedding and storage of Python files' content into a ChromaDB database. The primary functionalities focus on initializing a database client, managing collections (retrieving or creating them), embedding text documents, and storing relevant summaries and contexts for future retrieval. Moreover, it integrates OpenAI's embedding functionalities and includes extensive logging for robust error tracking and information reporting. \n\n### Main Functionalities\n\n1. **Database Initialization**: Functions to establish a connection to ChromaDB, ensuring that subsequent operations can interact with the database.\n\n2. **Collection Management**: Functions that handle the retrieval or creation of collections in ChromaDB, which organize stored data effectively.\n\n3. **Embedding and Storage**: The ability to read Python files, embed their contents, and store them within a specified ChromaDB collection for later use.\n\n4. **Context Retrieval**: A function to query the database for relevant documents based on specific criteria and return the accumulated context while adhering to token limits.\n\n5. **Storing Class Summaries**: Functions to embed summaries of classes and associate them with their respective Python files and class names in the database.\n\n6. **Error Handling and Logging**: Comprehensive logging and error handling to monitor operations and diagnose issues effectively.\n\n### Purpose\n\nThe purpose of this Python file can be summarized as follows:\n- To manage linguistic embeddings of Python file content and organize it within a ChromaDB database, providing a structured approach to access and utilize contextual information.\n- To create a seamless integration between OpenAI\u2019s embedding models and ChromaDB, facilitating efficient data retrieval in various applications such as code analysis or documentation generation.\n- To offer robust error handling and logging for developers, enabling better maintainability and usability of the code.\n\n### Function Descriptions\n\nThe file includes several key functions, each with specific responsibilities:\n\n1. **initialize_chroma() -> chromadb.Client**:\n   - Initializes and returns a ChromaDB client instance connected to a server running on localhost.\n   - **Returns**: An instance of `chromadb.Client` for database interactions.\n   - **Example**: `client = initialize_chroma()`\n\n2. **get_or_create_collection(client: chromadb.Client, collection_name: str) -> chromadb.Collection**:\n   - Retrieves an existing collection by name or creates a new one if it does not exist.\n   - **Args**:\n     - `client`: ChromaDB client instance.\n     - `collection_name`: The name of the desired collection.\n   - **Returns**: The `chromadb.Collection` instance.\n   - **Raises**: Exception if there are issues retrieving or creating the collection.\n\n3. **embed_and_store_files(collection: chromadb.Collection, python_files: List[str], tags: Dict[str, str] = {}) -> None**:\n   - Reads Python files, embeds their content, and stores the representations in the specified ChromaDB collection.\n   - **Args**:\n     - `collection`: The ChromaDB collection instance for storage.\n     - `python_files`: A list of paths to Python files.\n     - `tags`: Additional metadata to associate with stored documents.\n   - **Raises**: Exception if errors occur during file reading or storage operations.\n\n4. **get_relevant_context(collection: chromadb.Collection, classes: List[str], max_tokens: int, where: str = None) -> str**:\n   - Retrieves relevant documents from ChromaDB based on class dependencies and ensures the returned context stays within a specified token limit.\n   - **Args**:\n     - `collection`: The ChromaDB collection to query.\n     - `classes`: A list of class names to use as query text.\n     - `max_tokens`: The maximum allowed token count for the context.\n   - **Returns**: A string containing the accumulated context.\n   - **Example**: `context = get_relevant_context(collection, classes, max_tokens)`\n\n5. **store_class_summary(collection: chromadb.Collection, file_path: str, class_name: str, summary: str) -> None**:\n   - Embeds a class's summary and stores it in the ChromaDB collection with appropriate metadata.\n   - **Args**:\n     - `collection`: The ChromaDB collection where the summary will be stored.\n     - `file_path`: The path to the associated file.\n     - `class_name`: The name of the class for which the summary is stored.\n     - `summary`: The summary text to store.\n   - **Raises**: Exception if errors occur while storing the summary.\n\n### Classes\n\nNo classes are defined in this file; however, the functions leverage ChromaDB\u2019s `Client` and `Collection` types.\n\n### Structure and Intent\n\nThe structure of the module is organized around functionality, focusing on operations related to embedding and storing data. The import statements bring in necessary libraries, such as OpenAI for embeddings, ChromaDB for storage, and logging for monitoring and debugging.\n\n- **Error Handling**: Each function captures exceptions and logs them, ensuring that any issue can be traced back to its source through logging messages and stack traces. \n\n- **Modularity**: Each function serves a single purpose, making the code reusable and easier to maintain. \n\n### Conclusion\n\nIn summary, this Python module provides a comprehensive system for embedding, storing, and retrieving content from Python files using ChromaDB and OpenAI's embeddings. It incorporates robust logging and error handling, ensuring clear visibility and traceability throughout operations. This structured approach not only enhances the performance and usability of the application but also aids developers in managing and utilizing class summaries and contextual code information effectively."
  },
  {
    "file": "./docstring_ai/lib/utils.py",
    "description": "Here\u2019s a comprehensive and detailed description of the provided Python file:\n\n### Overview\n\nThis Python file serves as a utility tool primarily focused on managing Python code within a Git repository. It provides functions to check for uncommitted changes, ensure the presence of docstring headers in files, manage caching, compute hashes for file integrity, create backups, and display diffs between code versions. By integrating Git and filesystem operations, the code enhances the workflow of Python file management, particularly in the context of version control and code maintenance.\n\n### Main Functionalities\n\n1. **Docstring Management**: Ensures the presence of a standardized docstring header in Python files by checking for a specific tag.\n\n2. **Git Repository Management**: Checks if the specified directory is a Git repository and whether certain files within it have uncommitted changes. It provides functionalities to handle user confirmations when uncommitted changes are present.\n\n3. **File and Cache Operations**: Manages file retrieval and caching operations, including loading and saving cache information about the files.\n\n4. **File Integrity and Backup Management**: Computes SHA-256 hashes for files to check for changes and creates backups of files before modifications.\n\n5. **Differencing**: Displays differences between original and modified versions of code to facilitate code review processes.\n\n6. **Directory Traversal**: Facilitates directory navigation within the repository, categorizing folders based on their depth to aid in analysis or reporting.\n\n### Purpose\n\nThe purpose of this Python module can be summarized as follows:\n- To facilitate efficient management of Python files within a Git-controlled environment.\n- To enhance code maintenance practices through automated checks, docstring handling, change tracking, and backup creation.\n- To support developers in ensuring code quality and consistency by providing tools for version comparison and repository management.\n\n### Function Descriptions\n\nThe file comprises several well-defined functions:\n\n1. **ensure_docstring_header(content: str) -> str**:\n   - Ensures that the input string contains a docstring header, appending it if missing.\n   - **Args**: `content`: The content of the file.\n   - **Returns**: The modified content with the docstring header.\n   \n2. **file_has_uncommitted_changes(repo_path: str, file_path: str) -> bool**:\n   - Checks if the specified file has uncommitted changes using the `git diff` command.\n   - **Returns**: `True` if there are uncommitted changes, otherwise `False`.\n\n3. **prompt_user_confirmation(message: str) -> bool**:\n   - Prompts the user for confirmation and waits for \"yes\" or \"no\" responses.\n   - **Returns**: `True` if the user confirms, otherwise `False`.\n\n4. **check_git_repo(repo_path) -> bool**:\n   - Checks if the specified directory is a Git repository by executing a specific Git command.\n   - **Returns**: `True` if it's a Git repository, otherwise `False`.\n\n5. **has_uncommitted_changes(repo_path) -> bool**:\n   - Checks for any uncommitted changes in the whole repository and prompts the user for action if any are found.\n   - **Returns**: `True` if uncommitted changes are present, otherwise `False`.\n\n6. **load_cache(cache_file: str) -> Dict[str, str]**:\n   - Loads the cache from a specified JSON file into a dictionary.\n   - **Returns**: A dictionary mapping file paths to hash values or an empty dictionary if the file does not exist.\n\n7. **save_cache(cache_file: str, cache: Dict[str, str])**:\n   - Saves the cache dictionary into a JSON file.\n   - **Args**: `cache_file`: The filename for saving the cache data.\n\n8. **get_python_files(repo_path: str) -> List[str]**:\n   - Recursively retrieves all Python files in the specified repository.\n   - **Returns**: A list of file paths for all `.py` files found.\n\n9. **sort_files_by_size(file_paths: List[str]) -> List[str]**:\n   - Sorts given file paths in ascending order based on their file sizes.\n   - **Returns**: A list of file paths sorted by size.\n\n10. **compute_sha256(file_path: str) -> str**:\n    - Computes and returns the SHA-256 hash of the specified file.\n    - **Returns**: The computed hash as a hexadecimal string.\n\n11. **traverse_repo(repo_path: str, pr_depth: int) -> Dict[int, List[str]]**:\n    - Traverses the repository to categorize folders by their depth relative to the root.\n    - **Returns**: A dictionary mapping depth levels to lists of folder paths.\n\n12. **create_backup(file_path: str)**:\n    - Creates a backup of the specified file, appending a timestamp to the backup's filename.\n   \n13. **show_diff(original_code: str, modified_code: str) -> str**:\n    - Generates and returns a unified diff between two code string versions.\n    - **Returns**: A string representing the differences.\n\n### Structure and Intent\n\n- **Imports**: The file imports various modules that provide functionalities like file handling, Git commands, JSON management, logging, and time manipulation.\n\n- **Logging**: Throughout the functions, logging is used to output information or errors, which helps in debugging and tracking the application's behavior.\n\n- **Error Handling**: The functions expect various conditions (e.g., existence of files, status of Git repositories) and handle exceptions to ensure the application doesn\u2019t crash unexpectedly.\n\n### Conclusion\n\nIn summary, this Python module is a practical tool designed to enhance the management of Python code within Git repositories. Its functionalities cater to ensuring consistent coding practices, integrity checks through hashing, structured documentation via docstring headers, and the ability to efficiently back up changes. Through these features, the module aids developers in maintaining a clean, efficient workflow, contributing to improved code quality and easier project management."
  },
  {
    "file": "./docstring_ai/__main__.py",
    "description": "Here\u2019s a comprehensive and detailed description of the provided Python file, capturing its main functionalities, purpose, classes, and function constructors along with important context, structure, and intent.\n\n### Overview\n\nThis Python script automates the process of adding docstrings to Python files, integrating with GitHub to create pull requests (PRs) using OpenAI's API for generating the docstring content. It leverages various modules to handle command-line interface (CLI) interactions, file operations, environment variables, and GitHub API interactions.\n\n### Main Functionalities\n\n1. **Documenting Python Code**: The script automates the process of generating and adding docstrings to Python functions and classes based on code analysis and specified templates.\n\n2. **GitHub Integration**: It integrates with GitHub to facilitate the creation of pull requests for modified files, enabling version control and collaborative development.\n\n3. **API Interaction**: It interacts with OpenAI\u2019s API to generate meaningful docstring comments, enhancing the readability and maintainability of the code.\n\n4. **Command-Line Interface**: The script provides a CLI for users to specify input parameters such as paths, API keys, and repository details.\n\n5. **Error Handling and User Prompts**: It includes mechanisms for error handling, user confirmations, and checking conditions (like uncommitted changes) before proceeding with modifications.\n\n### Purpose\n\nThe purpose of this Python module can be summarized as follows:\n- To facilitate the documentation of Python codebases by automatically adding docstring headers to functions and classes.\n- To enhance collaboration by integrating the documentation process with GitHub, allowing developers to easily create pull requests for their changes.\n- To streamline code review and increase code quality through automated docstring generation.\n\n### Function Descriptions\n\nThe file includes several key functions aimed at accomplishing the defined tasks:\n\n1. **is_git_repo(folder_path)**:\n   - Determines if the specified folder is a Git repository using a `git` command.\n   - **Returns**: `True` if it is a repository, `False` otherwise.\n\n2. **get_remote_url(folder_path)**:\n   - Retrieves the remote URL for the Git repository.\n   - **Returns**: The remote URL as a string or `None` if it fails.\n\n3. **parse_github_url(remote_url)**:\n   - Extracts user and repository name from a GitHub remote URL.\n   - **Returns**: A tuple (user, repo).\n\n4. **determine_pr_target(path: str, args)**:\n   - Determines whether to enable PR creation and identifies the target GitHub repository.\n   - **Args**: Takes the repository path and parsed CLI arguments.\n   - **Returns**: A tuple indicating if PR creation is enabled and the GitHub repository string if applicable.\n\n5. **determine_target_branch(path: str, args)**:\n   - Determines the target branch for the PR based on the current Git branch or CLI arguments.\n   - **Returns**: The name of the target branch.\n\n6. **main()**:\n   - The primary entry point of the script. Sets up the CLI, validates arguments, and orchestrates the process of adding docstrings and integrating with GitHub.\n   - **Handles**:\n     - Argument parsing for user inputs.\n     - PR target and branch determination.\n     - Cleaning up caches if the `--no-cache` flag is used.\n     - Executes the process_files_and_create_prs function to handle file processing and GitHub integration.\n\n### Structure and Intent\n\n- **Imports**: The script imports relevant libraries for various functionalities such as handling GitHub interactions, command-line argument parsing, file I/O operations, and environmental variable management.\n\n- **Logging**: The script maintains logging for tracking execution steps, informing users of actions taken, and logging any errors encountered during execution.\n\n- **Setup and Configuration**: It utilizes environment variables loaded from a `.env` file for sensitive information like API keys, contributing to better security practices.\n\n- **User Prompts**: The script engages users through prompts, ensuring that critical actions (like proceeding with changes) are confirmed, thus reducing the risk of unintended modifications.\n\n### Conclusion\n\nIn summary, this Python module is a comprehensive and automated tool designed to streamline the process of documenting Python codebases by adding docstrings and integrating these modifications with GitHub through pull requests. It enhances the developer experience by combining functionalities for code documentation, version control, and user interaction in a single automated workflow. This results in improved code quality and easier collaboration in software development projects, significantly aiding developers in maintaining and documenting their Python projects efficiently."
  },
  {
    "file": "./docstring_ai/lib/docstring_utils.py",
    "description": "Here\u2019s a comprehensive and detailed description of the provided Python file, capturing its functionalities, purpose, classes, and function constructors, as well as important context and intent.\n\n### Overview\n\nThis Python module is designed to facilitate the extraction and manipulation of docstrings from Python code. The module includes functions for parsing Python classes, extracting descriptions from docstrings, adding docstrings through the OpenAI API, and logging errors that may arise during execution. It utilizes the Abstract Syntax Tree (AST) to analyze Python code structure and content, significantly enhancing code documentation practices.\n\n### Main Functionalities\n\n1. **Docstring Extraction**: Functions to extract and compile descriptions from docstrings present in functions, classes, and modules.\n\n2. **Class Parsing**: Ability to parse Python files to identify classes and their parent classes, allowing for better organization and understanding of class hierarchies.\n\n3. **Logging Mechanism**: Integrated logging for debugging and error reporting purposes, which helps in tracking the status of operations and identifying failures.\n\n4. **AST Utilization**: Utilizes Python's AST library to analyze code structure, offering a programmatic way to traverse and manipulate Python code.\n\n5. **OpenAI API Interaction**: Although not displayed in the current functions, it has integrations indicative of future capabilities for enhancing docstring content with AI-generated suggestions through OpenAI's Assistant.\n\n### Purpose\n\nThe purpose of this module can be summarized as follows:\n- To automate the documentation process in Python codebases by extracting existing docstrings and ensuring proper documentation standards.\n- To provide utilities for better understanding the structure of Python code, especially regarding classes and their relationships.\n- To streamline the interaction between developers and OpenAI's Assistant for enhancing docstring content dynamically as needed.\n\n### Function Descriptions\n\nThe file contains several well-defined functions aimed at achieving the aforementioned purposes:\n\n1. **extract_description_from_docstrings(code_with_docstrings: str) -> str**:\n   - Parses provided Python code and extracts the first line of docstrings, returning them as a semicolon-separated string.\n   - **Args**: `code_with_docstrings`: The Python code containing docstrings.\n   - **Returns**: A string of extracted descriptions.\n   - **Raises**: Exception if there are errors during parsing.\n\n2. **extract_class_docstring(code: str, class_name: str) -> str**:\n   - Extracts the docstring of a specified class from the provided Python code.\n   - **Args**: `code`: The code containing class definitions; `class_name`: The target class name.\n   - **Returns**: The class docstring or an empty string if not found.\n   - **Raises**: Exception for errors encountered during extraction.\n\n3. **parse_classes(file_path: str) -> Dict[str, List[str]]**:\n   - Reads and parses a Python file to construct a dictionary mapping class names to their parent classes.\n   - **Args**: `file_path`: Path to the Python file to analyze.\n   - **Returns**: A dictionary of classes and their parent classes.\n   - **Raises**: Exception for errors during reading or parsing.\n\n### Class: DocstringExtractor\n\nThe module defines the `DocstringExtractor` class, which encapsulates methods for extracting docstrings and analyzing Python files.\n\n#### Class Description\n\n```python\nclass DocstringExtractor:\n    \"\"\"\n    A class to extract all docstrings from a Python file and compile them into a readable format.\n    \"\"\"\n\n    def __init__(self, file_path: str):\n        \"\"\"\n        Initializes the extractor with the path to the Python file.\n\n        Args:\n            file_path (str): The path to the Python script to be analyzed.\n        \"\"\"\n        self.file_path = file_path\n        self.file_content: Optional[str] = None\n        self.tree: Optional[ast.AST] = None\n        self.docstrings: Dict[str, Dict[str, str]] = {}\n        self.imports: Dict[str, List[str]] = {}\n```\n\n#### Constructor\n\n- **`__init__`**: Initializes the `DocstringExtractor` with the path to the Python file, setting up attributes for file content, AST tree, docstrings, and imports.\n\n#### Key Methods\n\n1. **read_file() -> None**:\n   - Reads the content of the specified Python file into memory.\n   - **Raises**: FileNotFoundError and IOError for issues during file access.\n\n2. **parse_ast() -> None**:\n   - Parses the read file content into an Abstract Syntax Tree (AST).\n   - **Raises**: SyntaxError if the code has invalid syntax, and ValueError if the file content is not set.\n\n3. **extract_docstrings() -> None**:\n   - Extracts docstrings from the AST and populates the `docstrings` dictionary.\n   - Gathers module-level docstring and those associated with classes and functions.\n\n4. **list_imports_from_package(package: str) -> List[str]**:\n   - Scans the AST for imports from a specified package.\n   - **Args**: `package`: The package to extract imports from.\n   - **Raises**: ValueError if the AST hasn't been parsed.\n\n5. **compile() -> str**:\n   - Combines the extracted docstrings into a reader-friendly text format.\n   - **Returns**: String that summarizes the docstrings.\n\n6. **get_docstrings_dict() -> Dict[str, Dict[str, str]]**:\n   - Returns the dictionary of extracted docstrings.\n\n7. **process() -> Dict[str, Dict[str, str]]**:\n   - High-level function that handles the entire process of reading the file, parsing the AST, and extracting docstrings.\n   - **Returns**: The dictionary of extracted docstrings.\n\n8. **process_imports(package: str) -> List[str]**:\n   - High-level method to list all imports for a specified package.\n   - **Args**: `package`: The target package for import extraction.\n   \n### Logging Configuration\n\n- The module utilizes Python\u2019s built-in logging module to track execution through logging statements at different levels, including `info`, `warning`, and `error`, ensuring developers can debug effectively.\n\n### Conclusion\n\nIn summary, this file provides a robust framework for automating docstring extraction and management in Python code. Through the use of AST for code analysis, the module not only enhances documentation practices but also consolidates knowledge about Python class structures. The integration with OpenAI\u2019s API indicates potential future features aimed at enriching the generated docstrings, thus streamlining the documentation process even further. This module is invaluable for projects where consistent documentation is essential for maintenance and readability."
  },
  {
    "file": "./docstring_ai/lib/github_utils.py",
    "description": "Here's a comprehensive and detailed description of the provided Python file, emphasizing its main functionalities, purpose, classes, function constructors, and important details that help understand the intent and structure of the code.\n\n### Overview\n\nThis Python module provides functionality for integrating with the GitHub API to automate the process of creating pull requests (PRs). Specifically, it focuses on managing branches, committing changes, and generating pull requests that incorporate modifications made to Python files in a local Git repository. It utilizes a number of libraries for handling file operations, Git repository interactions, and logging.\n\n### Main Functionalities\n\n1. **Branch Management**: Functions to sanitize branch names, create and switch branches, and check out specified branches.\n\n2. **Change Detection**: Ability to retrieve a list of changed Python files between branches for inclusion in PRs.\n\n3. **GitHub Integration**: Automates the tasks of interacting with the GitHub API, including creating pull requests, making commits, and pushing changes to remote repositories.\n\n4. **File Handling**: Functions to traverse directories and collect relevant files (Python files in this case).\n\n5. **Logging**: Integrated logging functionality to provide detailed output on operations, successes, and errors encountered during execution.\n\n### Purpose\n\nThe primary purpose of this module can be summarized as:\n- To facilitate the automation of committing and pushing changes to a GitHub repository, including the generation of pull requests for modifications made by developers. This streamlines workflow in collaborative development environments by reducing manual operations and ensuring consistency in version control tasks.\n\n### Function Descriptions\n\nThe file contains several key functions aimed at accomplishing these tasks:\n\n1. **sanitize_branch_name(name: str) -> str**:\n   - Sanitizes and formats a branch name by replacing invalid characters with underscores and flattening any hierarchy by replacing slashes with dashes.\n   - **Returns**: A sanitized string for the branch name.\n\n2. **generate_unique_suffix() -> str**:\n   - Generates a unique suffix (8 characters) using UUID4 for branch naming.\n   - **Returns**: A unique string suffix.\n\n3. **create_github_pr(repo_path: str, github_token: str, github_repo: str, branch_base_name: str, pr_name: str, target_branch: str) -> bool**:\n   - Automates the creation of a GitHub pull request, including committing changes and pushing them to a new branch on GitHub.\n   - This function handles:\n     - Branch creation\n     - Changes commit and push\n     - Pull request creation with details of changed files.\n   - **Returns**: True if the PR was created successfully, otherwise False.\n\n4. **checkout_branch(repo_path: str, branch_name: str) -> bool**:\n   - Checks out the specified branch in a local Git repository.\n   - **Returns**: True if the checkout was successful, otherwise False.\n\n5. **get_python_files(repo_path: str) -> List[str]**:\n   - Retrieves and returns a list of all Python files within the specified repository directory.\n   - **Returns**: A list of strings, each representing the path to a Python file.\n\n6. **create_pull_request_body(changed_files: List[str]) -> str**:\n   - Constructs the body content for the pull request, listing the changed Python files.\n   - **Returns**: A string formatted for the pull request description.\n\n7. **commit_and_push_changes(repo_path: str, branch_name: str, commit_message: str) -> bool**:\n   - Manages Git operations to add changes, commit them, and push to a specified branch.\n   - **Returns**: True if the operations are successful, otherwise False.\n\n8. **get_changed_files(repo_path: str, branch_name: str, base_branch: str) -> List[str]**:\n   - Retrieves all changed Python files between the provided feature branch and base branch.\n   - **Returns**: A list of paths to changed Python files.\n\n9. **log_git_status(repo_path: str) -> bool**:\n   - Logs the current Git status of the repository.\n   - **Returns**: True if the status was logged successfully, otherwise False.\n\n### Structure and Context\n\n- **Imports**: The module imports various libraries required for file handling, Git operations, logging, and managing GitHub interactions. The use of libraries such as `os`, `argparse`, and `subprocess` plays a crucial role in command execution and filesystem manipulation.\n\n- **Logging Setup**: Throughout the module, logging is standardized to capture information at various steps, aiding in debugging and operational oversight.\n\n- **Error Handling**: The module contains comprehensive error handling, especially around Git operations, ensuring that any failures during branch checks, commits, and pushes are logged and handled appropriately.\n\n### Conclusion\n\nIn summary, this Python module is designed to streamline the process of managing pull requests in a GitHub repository by automating branch creation, file change detection, committing, and PR generation. It greatly simplifies a developer's workflow in collaborative environments by reducing the need for manual git operations and ensuring consistent PR processes. With proper logging and error management, the module provides a robust solution for integrating code changes with GitHub efficiently."
  },
  {
    "file": "./docstring_ai/lib/prompt_utils.py",
    "description": "Here's a comprehensive and detailed description of the provided Python file, emphasizing its main functionalities, purpose, classes, function constructors, and important contextual information.\n\n### Overview\n\nThis Python module facilitates the operation of an AI assistant specifically designed to generate and add docstrings to Python code. It leverages OpenAI's services to create and manage an assistant, handle prompts, and interact with code context stored in ChromaDB. The module includes utilities for managing threads of interaction, constructing few-shot prompts for effective AI responses, and updating the assistant's resources.\n\n### Main Functionalities\n\n1. **Assistant Management**: Functions to initialize, retrieve, and manage the assistant that operates based on OpenAI\u2019s API, ensuring it is set up to handle requests for adding docstrings.\n\n2. **Prompt Construction**: Capability to create prompts using existing context data and few-shot examples to generate quality responses from the AI assistant.\n\n3. **Thread Management**: Functions for creating threads to maintain conversation contexts with the assistant, enabling ongoing exchanges without losing context.\n\n4. **File Handling**: Use of ChromaDB to retrieve relevant context about Python code, and operations to generate and manage docstrings effectively.\n\n5. **Error Handling and Logging**: Integrated logging for debugging and tracing the execution flow; it captures errors or misconfigurations that might arise during API interactions.\n\n### Purpose\n\nThe purpose of this module can be summarized as follows:\n- To automate the process of enhancing Python code documentation through the use of an AI assistant that generates comprehensive and context-aware docstrings.\n- To manage interactions with OpenAI\u2019s API and ChromaDB to ensure that the context and content generated are relevant and tailored to the specific needs of the code being documented.\n- To streamline workflows for developers, enabling them to focus on core coding tasks while automatically improving documentation quality.\n\n### Class Description\n\n#### `PythonFile`\n\n```python\nclass PythonFile(BaseModel):\n    new_file_content: str = Field(description=\"Updated python script with the updated docstrings.\")\n```\n- **Purpose**: This class, defined using Pydantic's `BaseModel`, is designed to manage the file content for a Python script that has been updated to include new docstrings.  \n- **Attributes**:\n  - `new_file_content`: A string that holds the content of the updated Python script after the docstrings have been added.\n\n### Function Descriptions\n\nThe file includes several key functions aimed at achieving its goals:\n\n1. **initialize_assistant(api_key: str, assistant_name: str = \"DocstringAssistant\") -> str**:\n   - Initializes or retrieves an existing AI assistant.\n   - **Returns**: The ID of the assistant or `None` if an error occurs.\n\n2. **update_assistant_tool_resources(api_key: str, assistant_id: str, file_ids: List[str]) -> None**:\n   - Updates the assistant\u2019s resources with new file IDs, creating a vector store.\n   - **Raises**: Exception for errors updating resources.\n\n3. **create_thread(api_key: str, assistant_id: str, initial_messages: List[dict] = None) -> str**:\n   - Creates a new thread for communicating with the assistant.\n   - **Returns**: The ID of the created thread or `None` if an error occurs.\n\n4. **construct_few_shot_prompt(collection: chromadb.Collection, classes: Dict[str, List[str]], max_tokens: int, context: str = None) -> str**:\n   - Constructs a few-shot prompt using context summaries.\n   - **Returns**: A formatted string prompt for the assistant.\n\n5. **extract_code_from_message(message: str) -> str**:\n   - Extracts code blocks from the assistant's messages using a regular expression.\n   - **Returns**: The extracted code block or raises an error if none exists.\n\n6. **send_message_to_assistant(assistant_id: str, thread_id: str, prompt: str, response_format: BaseModel = None, tools: List = [], tool_choice=\"auto\", functions: Dict[str, Callable] = {}) -> str**:\n   - Sends a message to the assistant and retrieves a response.\n   - **Returns**: The assistant's response text or an error message if an issue occurs.\n\n7. **generate_file_description(assistant_id: str, thread_id: str, file_content: str) -> str**:\n   - Generates a detailed description of a Python file using the assistant.\n   - **Returns**: A string containing the file description.\n\n8. **create_file_with_docstring(assistant_id: str, thread_id: str, code: str, context: str, functions: Dict[str, Callable]) -> str**:\n   - Requests the assistant to add docstrings to the provided code.\n   - **Returns**: The code with added docstrings, or `None` if an error occurs.\n\n9. **create_vector_store(vector_store_name: str, file_ids: List[str]) -> str**:\n   - Creates a vector store in ChromaDB and associates it with file IDs.\n   - **Returns**: The ID of the created vector store.\n\n10. **poll_run_completion(run_id: str, thread_id: str, functions: Dict[str, Callable]) -> bool**:\n    - Polls until the completion of a run, implementing a retry mechanism.\n    - **Returns**: `True` if successful; otherwise, `False`.\n\n11. **retrieve_last_assistant_message(thread_id: str) -> str**:\n    - Retrieves the last message from a thread to maintain context in conversations.\n    - **Returns**: The last message text or `None` if no messages exist.\n\n### Context and Structure\n\n- **Imports**: The module imports numerous libraries to handle API interactions, logging, data manipulation, and type-checking with Pydantic.\n\n- **Error Handling**: Extensive error handling incorporates logging statements to provide insights into possible failures during execution, making it easier to debug.\n\n- **Prompt Management**: The use of few-shot prompting demonstrates an innovative way to generate high-quality responses from AI, leveraging prior examples to guide the assistant\u2019s output.\n\n### Conclusion\n\nIn summary, this Python module serves as a powerful tool for automating the documentation of Python projects through the interaction of an AI assistant with developers. By integrating OpenAI\u2019s API and managing data contexts from ChromaDB, the module streamlines the process of adding comprehensive docstrings to Python code. Its design emphasizes usability through structured functions, effective error handling, and a logging system that captures crucial information about actions taken and any issues encountered. This tool is invaluable for enhancing code quality and ensuring that documentation is clear and informative."
  },
  {
    "file": "./docstring_ai/lib/process.py",
    "description": "Here's a comprehensive and detailed description of the provided Python file, focusing on its main functionalities, purpose, classes, function constructors, and other relevant contextual information.\n\n### Overview\n\nThis Python module automates the process of enhancing Python code documentation by generating and adding docstrings using OpenAI's Assistant. It further integrates with ChromaDB for contextual keyword storage and GitHub for creating pull requests. The file includes several functions for file processing, API communication, update management, and documentation generation.\n\n### Main Functionalities\n\n1. **Docstring Addition**: The module uses OpenAI's API to generate docstrings based on the context of existing Python code.\n\n2. **GitHub Integration**: It facilitates the creation of pull requests on GitHub after processing Python files, allowing for collaboration and version control.\n\n3. **Context Management**: The module utilizes ChromaDB to embed and retrieve contextual information related to the code being processed, aiming to generate accurate and informative docstrings.\n\n4. **File Management**: The module can traverse directories to find Python files, verify their unique identifiers through SHA-256 hashes, and manage file updates and backups.\n\n5. **Progress Tracking**: Incorporates visualization libraries like `tqdm` to show progress updates during file processing, improving user experience during long-running tasks.\n\n### Purpose\n\nThe primary purpose of this module is to provide an automated system for enhancing the documentation quality of Python codebases by adding comprehensive docstrings through the assistant's interactions with the code. This reduces the manual workload for developers while maintaining or improving code quality and documentation standards. Additionally, the integration with GitHub allows for seamless collaboration.\n\n### Function Descriptions\n\nThe file consists of several key functions outlined below:\n\n1. **process_files_and_create_prs(repo_path: str, api_key: str, create_pr: bool, github_token: str, github_repo: str, branch_name: str, pr_name: str, pr_depth: int, manual: bool, target_branch: str) -> None**:\n   - Processes Python files, adds docstrings, and creates pull requests on GitHub if specified.\n   - **Args**: Paths, API keys, and PR specifications.\n   - **Raises**: Various exceptions during processing, logged appropriately.\n\n2. **process_single_file(python_file_path: str, repo_path: str, assistant_id: str, thread_id: str, collection, context_summary: list, cache: dict, manual: bool) -> None**:\n   - Processes a single Python file to generate and add docstrings.\n   - Handles reading, modifying, and saving files, as well as manual approval and context updates.\n\n3. **filter_files_by_hash(file_paths: List[str], repo_path: str, cache: Dict[str, str]) -> List[str]**:\n   - Filters files based on their SHA-256 hash to determine which files need processing.\n   - **Returns**: List of files to be processed.\n\n4. **upload_files_to_openai(file_paths: List[str]) -> List[str]**:\n   - Uploads Python files to OpenAI for further processing and returns their file IDs.\n   - **Returns**: List of file IDs uploaded.\n\n5. **create_github_pr(...)**:\n   - Creates a pull request on GitHub incorporating changes made to files.\n\n6. **get_python_files(repo_path: str) -> List[str]**:\n   - Retrieves a list of all Python files in the specified directory.\n   - **Returns**: List of relative paths of Python files.\n\n7. **create_pull_request_body(changed_files: List[str]) -> str**:\n   - Creates the body content for the pull request, listing all modified files.\n\n### Contextual Information\n\n- **Imports**: The module imports needed libraries such as `openai`, `chromadb`, and `tqdm` for API interactions and progress tracking, and standard libraries like `os`, `json`, and `logging` for general operations.\n\n- **Logging**: The module uses the `logging` library extensively for debugging and tracking errors, which aids in maintaining clear operation traces throughout the processing.\n\n- **Caching**: The implementation includes caching mechanisms to track file states across runs, reducing unnecessary processing on files that have not changed.\n\n- **Error Handling**: Each function is equipped with error handling that logs issues encountered during execution, which is crucial for debugging and maintaining the module.\n\n### Structure\n\nThe structure of the module is organized around the main processing function, `process_files_and_create_prs`, which orchestrates the overall workflow, making it easy to understand and maintain. Helper functions are succinctly defined to handle file management and GitHub-related operations, maintaining a modular design.\n\n### Conclusion\n\nIn summary, this Python module plays a critical role in automating the generation of docstrings for Python files, utilizing OpenAI's capabilities in a structured workflow. By integrating with ChromaDB for context retrieval and facilitating easy GitHub pull request creation, it streamlines the documentation process in software development. The careful implementation of error handling, caching, and logging enhances its reliability and effectiveness in real-world usage. This module is particularly valuable for teams aiming to maintain high standards in documentation and code quality while minimizing manual effort."
  },
  {
    "file": "docstring_ai/lib/config.py",
    "description": "Here's a comprehensive overview of the provided Python file, outlining its main functionalities, purpose, classes, function constructors, and other significant details.\n\n### Overview\n\nThis Python file is structured to set up configurations for a natural language processing (NLP) service that utilizes specific machine learning models from OpenAI (e.g., GPT) and embedding techniques. It includes logical constants, logging configurations, and a custom logging formatter to enhance readability and maintenance. The constants define various operational parameters, while the logging setup facilitates effective monitoring of the application's behavior.\n\n### Main Functionalities\n\n1. **Model Configuration**: Constants are defined to specify model names, maximum token limitations, and data paths. These facilitate easy modification based on the processing needs.\n2. **Caching Mechanism**: Paths for cache and summary files are established to improve performance during data retrieval and processing while ensuring persistence.\n3. **Logging Configuration**: Custom logging setup employs colored output based on severity levels to enhance the clarity of log messages and includes filters to exclude logs from certain libraries.\n\n### Constants\n\n- **MODEL**: Defines the name of the model used for task processing (`gpt-4o-mini`).\n- **MAX_TOKENS**: Sets the maximum number of tokens allowed in a single request (64000), ensuring compliance with model constraints.\n- **EMBEDDING_MODEL**: Specifies the OpenAI model for generating text embeddings (`text-embedding-3-large`).\n- **MAX_RETRIES**: Limits the number of retries for failed API requests (5) to increase resilience.\n- **RETRY_BACKOFF**: Defines the time to wait before retrying after a failed request (5 seconds).\n- **CHROMA_COLLECTION_NAME**: Identifies the ChromaDB collection for storing context data (`python_file_contexts`).\n- **DATA_PATH**: Establishes the base path for data storage (`./data/`).\n- **CACHE_FILE_NAME**: Specifies the filename for caching data (`docstring_cache.json`).\n\n### File Paths Related to Context\n\n- **CONTEXT_SUMMARY_PATH**: Path for storing context summaries (`context_summary.json`).\n- **DOCSTRING_AI_TAG**: A metadata tag indicating the origin of the docstrings generated.\n\n### Logging Configuration\n\nThe logging section of the file includes several classes and functions to configure how logs are generated and displayed:\n\n1. **ColoredFormatter**: \n   - A subclass of `logging.Formatter` that adds color coding to log messages based on their severity (DEBUG, INFO, WARNING, ERROR, CRITICAL).\n   - The `format` method formats log records, assigning colors according to the severity level.\n\n2. **ExcludeLibrariesFilter**: \n   - A filter class derived from `logging.Filter`, which suppresses logs from specified modules to avoid clutter in the output.\n   - It checks if the log record name starts with any of the excluded module names.\n\n3. **HTTPRequestFilter**:\n   - Another filter class that removes logs containing 'HTTP Request:' from the output, keeping the logs clean and focused on significant application events.\n\n4. **setup_logging()**:\n   - A function to set up the logging configuration, including creating a colored formatter, console handler, and establishing the log level and handler.\n   - It applies the `ExcludeLibrariesFilter` and `HTTPRequestFilter` to the root logger to filter out unnecessary logging output.\n\n### Purpose and Context\n\nThe primary purpose of this code is to create a robust framework for an NLP application using OpenAI's models. The file effectively separates configuration details (constants and paths) from logging and operational logic, adhering to coding best practices. This modular design allows for maintainability and scalability, enabling developers to adapt the constants and logging behavior as the application evolves or as new requirements emerge.\n\n### Intent of the Code\n\nThe intent behind defining constants for models, paths, caching, and implementing a custom logging structure is to create a reliable and efficient NLP processing environment. By providing a standardized way to handle logging and ensuring configurations are clearly defined, the code enhances usability and supports developers in building, debugging, and extending the NLP application with minimal friction.\n\nOverall, the file serves as a foundational component in an application that leverages machine learning models for natural language understanding and processing, showcasing efficient handling of configurations and logging mechanisms."
  },
  {
    "file": "docstring_ai/lib/chroma_utils.py",
    "description": "The provided Python file is designed to work with the ChromaDB database, OpenAI's API, and various Python file processing tasks. The primary function of this code revolves around embedding Python files, storing their representations in a database, and retrieving relevant information based on class dependencies within the files. Below is a comprehensive and detailed description of the code, highlighting its main functionalities, purpose, classes, and function constructors.\n\n### Overview\n\nThe file contains functions for:\n- Initializing a ChromaDB client.\n- Creating or retrieving collections in ChromaDB.\n- Embedding the contents of Python files and storing them in ChromaDB.\n- Retrieving context based on class dependencies while respecting token limits.\n- Storing summaries of classes in ChromaDB.\n\n### Main Functionalities\n\n1. **Database Initialization**: Establishes a connection to ChromaDB sever.\n2. **Collection Management**: Fetches existing collections or creates new ones as necessary.\n3. **File Processing**: Reads Python files, embeds their content, and stores embeddings along with metadata.\n4. **Context Retrieval**: Gathers relevant embedded documents based on input classes, ensuring token limits are adhered to.\n5. **Summary Storage**: Allows for the storage of class summaries in the ChromaDB for later retrieval.\n\n### Functions\n\nThis file consists of several defined functions, each with a specific purpose:\n\n1. **initialize_chroma()**\n   - **Purpose**: Establishes a connection to the ChromaDB server.\n   - **Returns**: A `chromadb.Client` instance.\n   - **Example Usage**:\n     ```python\n     client = initialize_chroma()\n     ```\n\n2. **get_or_create_collection(client: chromadb.Client, collection_name: str)**\n   - **Purpose**: Checks if a collection exists and retrieves it, otherwise creates a new collection.\n   - **Args**:\n     - `client`: The client used to interact with ChromaDB.\n     - `collection_name`: The name of the collection to check or create.\n   - **Returns**: A `chromadb.Collection` instance.\n   - **Raises**: Exceptions on failure to retrieve or create the collection.\n   - **Logging**: Provides feedback on whether the collection was found or created.\n\n3. **embed_and_store_files(collection: chromadb.Collection, python_files: List[str], tags: Dict[str, str] = {})**\n   - **Purpose**: Reads specified Python files, embeds their content, and stores them in the provided collection.\n   - **Args**:\n     - `collection`: The ChromaDB collection to store embedded documents.\n     - `python_files`: List of Python file paths to be embedded.\n     - `tags`: Optional additional metadata to associate with the embedded documents.\n   - **Raises**: Exceptions if file reading or storage fails.\n   - **Validation**: Checks for length mismatches, empty documents, and duplicate IDs before storing.\n\n4. **get_relevant_context(collection: chromadb.Collection, classes: List[str], max_tokens: int, where: str = None)**\n   - **Purpose**: Retrieves relevant documents based on the class dependencies while adhering to a token limit.\n   - **Args**:\n     - `collection`: The ChromaDB collection to query.\n     - `classes`: List of class names to fetch dependencies for.\n     - `max_tokens`: Maximum number of tokens allowed in the returned context.\n     - `where`: Optional filter criteria for querying if needed.\n   - **Returns**: A string containing the accumulated context.\n   - **Example Usage**:\n     ```python\n     context = get_relevant_context(collection, classes, max_tokens)\n     ```\n   - **Logging**: Logs the process of accumulating context and errors as needed.\n\n5. **store_class_summary(collection: chromadb.Collection, file_path: str, class_name: str, summary: str)**\n   - **Purpose**: Embeds and stores a summary of a specific class in the specified collection.\n   - **Args**:\n     - `collection`: The ChromaDB collection for storage.\n     - `file_path`: The file path where the class is located.\n     - `class_name`: The name of the class to summarize.\n     - `summary`: The summary text to store.\n   - **Raises**: Exceptions if there's an error storing the summary.\n   - **Logging**: Provides feedback when summaries are successfully stored or if there are errors.\n\n### Import Statements\n\n- **Libraries Imported**:\n  - `os`: Used for handling file paths.\n  - `openai`: Interacts with OpenAI\u2019s services for embedding.\n  - `logging`: For logging messages throughout the script to monitor its behavior.\n  - `chromadb`: To manage interactions with the ChromaDB database.\n  - `tiktoken`: Used for encoding and counting tokens within texts.\n  - `typing`: Provides type hints for better code clarity.\n  - `traceback`: To capture and log detailed error stack traces.\n  - The `EMBEDDING_MODEL` constant is imported from `docstring_ai` to specify which embedding model to use.\n\n### Purpose and Context\n\nThe code is intended for developers or data scientists working with Python codebases who need to extract meaningful representations from Python files for tasks such as:\n- Code summarization.\n- Automatic documentation.\n- Class dependency retrieval.\n\n### Intent of the Code\n\nThe overall intent of the code is to provide a framework for effectively managing embeddings of Python files and allowing for contextual retrieval based on those embeddings. The functions are designed to ensure easy modification and scalability while maintaining robust error handling and logging practices, facilitating future extensions or integrations with similar applications.\n\n### Conclusion\n\nIn summary, the provided Python file outlines a structured approach for initializing a ChromaDB client, managing collections, embedding Python files, and retrieving relevant data based on class dependencies. It emphasizes modularity, clear functionality, and efficient error handling, making it suitable for extending the capabilities of a programming-related NLP or AI application."
  },
  {
    "file": "docstring_ai/lib/utils.py",
    "description": "The provided Python file is designed to facilitate scripting workflows with a particular focus on managing Python files within a Git repository. It includes functions for handling various tasks such as ensuring code quality, generating backups, checking for uncommitted changes, and even creating diffs between original and modified code. Below is a comprehensive description of the code, highlighting its main functionalities, structure, purpose, and function constructors.\n\n### Overview\n\nThis script serves multiple purposes in a developer's workflow, especially when dealing with Python projects stored in Git repositories. It includes capabilities for:\n- Managing Git repository interactions.\n- Ensuring code documentation consistency.\n- Tracking changes and creating backups.\n- Comparing different versions of files.\n\n### Main Functionalities\n\n1. **Docstring Management**: Ensures that Python files contain specific documentation headers.\n2. **Git Integration**: Checks for the existence of a Git repository, confirms the presence of uncommitted changes, and interacts with repository features.\n3. **File Management**: Handles file processing tasks including hashing, retrieving Python files, sorting files by size, and creating backups.\n4. **Change Tracking**: Generates diffs between different versions of code files for inspection.\n\n### Functions\n\nThe code consists of several utility functions, each serving a specific purpose:\n\n1. **ensure_docstring_header(content: str) -> str**\n   - **Purpose**: Ensures a docstring header, defined by a constant `DOCSTRING_AI_TAG`, is present in the file content.\n   - **Args**: \n     - `content`: The content to check and modify if necessary.\n   - **Returns**: The modified content with the docstring header included if it was previously absent.\n\n2. **file_has_uncommitted_changes(repo_path: str, file_path: str) -> bool**\n   - **Purpose**: Checks if the specified file has uncommitted changes in a Git repository.\n   - **Args**: \n     - `repo_path`: The path to the Git repository.\n     - `file_path`: The path to the specific file.\n   - **Returns**: True if there are uncommitted changes; otherwise, False.\n\n3. **prompt_user_confirmation(message: str) -> bool**\n   - **Purpose**: Prompts the user for a binary confirmation (yes/no).\n   - **Args**: \n     - `message`: The message displayed to the user.\n   - **Returns**: True for 'yes', False for 'no'.\n\n4. **check_git_repo(repo_path: str) -> bool**\n   - **Purpose**: Determines whether the specified path is a Git repository.\n   - **Args**: \n     - `repo_path`: The directory path to check.\n   - **Returns**: True if the directory is a Git repository; otherwise, False.\n\n5. **has_uncommitted_changes(repo_path: str) -> bool**\n   - **Purpose**: Checks for any uncommitted changes in the repository and prompts the user if found.\n   - **Args**: \n     - `repo_path`: The Git repository path.\n   - **Returns**: True if uncommitted changes exist; False otherwise.\n\n6. **load_cache(cache_file: str) -> Dict[str, str]**\n   - **Purpose**: Loads the cache from a given JSON file.\n   - **Args**: \n     - `cache_file`: The path to the cache JSON file.\n   - **Returns**: A dictionary mapping file paths to their SHA-256 hashes.\n\n7. **save_cache(cache_file: str, cache: Dict[str, str])**\n   - **Purpose**: Saves a cache dictionary to a specified JSON file.\n   - **Args**: \n     - `cache_file`: Path to the cache file.\n     - `cache`: The dictionary to save.\n\n8. **get_python_files(repo_path: str) -> List[str]**\n   - **Purpose**: Recursively retrieves all Python files in the specified repository.\n   - **Args**: \n     - `repo_path`: The path to the Git repository.\n   - **Returns**: A list of paths to the Python files.\n\n9. **sort_files_by_size(file_paths: List[str]) -> List[str]**\n   - **Purpose**: Sorts files based on their size in ascending order.\n   - **Args**: \n     - `file_paths`: List of file paths to sort.\n   - **Returns**: A sorted list of file paths.\n\n10. **compute_sha256(file_path: str) -> str**\n    - **Purpose**: Computes the SHA-256 hash of a given file.\n    - **Args**: \n      - `file_path`: The path of the file to hash.\n    - **Returns**: The hexadecimal SHA-256 hash string.\n\n11. **traverse_repo(repo_path: str, pr_depth: int) -> Dict[int, List[str]]**\n    - **Purpose**: Categorizes folders based on their depth in the repository.\n    - **Args**:\n      - `repo_path`: The path to the repository.\n      - `pr_depth`: The maximum depth to categorize.\n    - **Returns**: A dictionary mapping depths to lists of folder paths.\n\n12. **create_backup(file_path: str)**\n    - **Purpose**: Creates a backup of the specified file with a timestamp.\n    - **Args**: \n      - `file_path`: The path of the file to back up.\n\n13. **show_diff(original_code: str, modified_code: str) -> str**\n    - **Purpose**: Generates a unified diff between the original and modified code.\n    - **Args**: \n      - `original_code`: The original code as a string.\n      - `modified_code`: The modified code as a string.\n    - **Returns**: A string containing the diff output.\n\n### Key Imports\n\nThe code utilizes several core libraries and modules:\n- **Standard Libraries**: \n  - `os`: For interacting with the operating system and file paths.\n  - `json`: For handling JSON data.\n  - `logging`: For logging events and errors.\n  - `argparse`: To handle command-line arguments.\n  - `subprocess`: For executing shell commands.\n  - `time`, `hashlib`: For timing operations and generating hashes.\n  - `datetime`: Provides classes for manipulating dates and times.\n  - `difflib`: For generating diffs of text.\n\n- **Third-Party Libraries**:\n  - `Github` and `GithubException`: For interfacing with Github repositories.\n  - `dotenv`: For loading environment variables from a `.env` file.\n  - `chromadb`: While imported, it doesn't seem to be actively used in this code snippet.\n  \n- **Local Imports**:\n  - `DOCSTRING_AI_TAG`, `DATA_PATH`: Constants probably defined in another module for configuration.\n\n### Purpose and Context\n\nThis code is primarily intended for developers who manage Python projects in Git repositories. It provides a suite of tools for maintaining code quality, ensuring consistency in documentation, and safely handling changes and backups. With its modular structure, the script can be extended or integrated into larger development workflows or continuous integration pipelines.\n\n### Intent of the Code\n\nThe intent here is to streamline development processes by:\n- Enforcing documentation standards in Python files.\n- Providing a mechanism to track and manage changes within Git repositories seamlessly.\n- Allowing developers to focus on coding while the script handles boilerplate maintenance tasks, such as backups and code comparison.\n\n### Conclusion\n\nOverall, the given Python file outlines a comprehensive utility for developers targeting Git-managed Python projects with features aimed at improving code governance and maintaining project integrity. Through thoughtful function designs and error handling, it encapsulates best practices for working with file systems and repositories in Python, making it an effective tool for software development workflows."
  },
  {
    "file": "docstring_ai/__main__.py",
    "description": "The provided Python file is a comprehensive script designed to automate the process of adding docstrings to Python files while integrating with GitHub for managing pull requests (PRs). It leverages multiple libraries and tools, including OpenAI's API for generating docstrings, and employs command-line interface (CLI) arguments for configuration. Below is a detailed description of the script's functionality, purpose, structure, and key components.\n\n### Overview\n\nThe script performs the following high-level operations:\n1. **Docstring Generation**: Automatically generates docstrings for Python files using OpenAI's API.\n2. **GitHub Integration**: Facilitates the creation of pull requests on GitHub to merge changes back into a designated repository.\n3. **CLI Interface**: Offers a command-line interface for users to customize the execution of the script.\n4. **File Management**: Handles cache management and automatic deletion of cached files if desired.\n\n### Main Functionalities\n\n1. **Automation of Docstring Addition**: The primary purpose of the script is to assist developers in adding meaningful docstrings to their Python codebase.\n2. **Pull Request Creation**: The script allows users to create pull requests to a GitHub repository, ensuring that the updated documentation can be easily reviewed and merged.\n3. **User Input Handling**: Through CLI arguments, users can customize paths, API keys, target branches, and more.\n4. **Validation and Confirmation Prompts**: The script includes mechanisms to ensure that users validate certain actions, such as modifying files or creating pull requests.\n\n### Function Descriptions\n\nThis file defines several functions, each aimed at performing specific tasks:\n\n1. **is_git_repo(folder_path)**\n   - **Purpose**: Checks if the specified directory is a Git repository.\n   - **Args**: \n     - `folder_path`: The path of the directory to check.\n   - **Returns**: True if it is a Git repository; otherwise, False.\n\n2. **get_remote_url(folder_path)**\n   - **Purpose**: Retrieves the remote URL for the Git repository.\n   - **Args**: \n     - `folder_path`: The path of the Git repository.\n   - **Returns**: The remote URL as a string, or None if not applicable.\n\n3. **parse_github_url(remote_url)**\n   - **Purpose**: Extracts the user and repository name from a GitHub remote URL.\n   - **Args**: \n     - `remote_url`: The remote URL of the repository.\n   - **Returns**: A tuple containing the user and repository name.\n\n4. **determine_pr_target(path: str, args)**\n   - **Purpose**: Determines whether to enable PR creation and identifies the target GitHub repository.\n   - **Args**: \n     - `path`: Path to the repository.\n     - `args`: CLI arguments.\n   - **Returns**: A tuple indicating whether PR creation is enabled and the corresponding GitHub repository.\n\n5. **determine_target_branch(path: str, args)**\n   - **Purpose**: Determines the target branch for the PR.\n   - **Args**: \n     - `path`: Path to the repository.\n     - `args`: CLI arguments.\n   - **Returns**: The name of the target branch as a string.\n\n6. **main()**\n   - **Purpose**: The entry point of the script. It manages CLI argument parsing, user interactions, and orchestrates the entire docstring generation and PR process.\n   - **Functionality**:\n     - Initializes the argument parser.\n     - Validates and processes command-line arguments.\n     - Manages GitHub integration based on user input.\n     - Invokes the `process_files_and_create_prs` function (imported from another module) to handle docstring addition and PR creation.\n\n### Key Imports\n\nThe script utilizes various modules and packages:\n\n- **GitHub Integration**: \n  - `github` for interacting with GitHub API for PR creation.\n  \n- **File Operations**:\n  - `pathlib` and `os` for file and directory handling.\n  \n- **API Interaction**:\n  - `openai` to call OpenAI's API for docstring generation.\n  \n- **Embedding Management**:\n  - `chromadb` for managing code context embeddings.\n\n- **Utilities**: \n  - `argparse` for command-line argument parsing.\n  - `dotenv` for loading environment variables from a `.env` file.\n  - `datetime` for date manipulation tasks.\n  - `logging` for logging messages, errors, and warnings.\n  - Other standard libraries for subprocess management and string parsing.\n\n### Purpose and Context\n\nThe script is tailored for software developers and engineers who want to improve the documentation quality of their codebases efficiently. By generating docstrings automatically and facilitating GitHub integration, it streamlines the workflow for code documentation. This becomes especially valuable in collaborative environments where clear documentation is crucial for maintainability and onboarding new team members.\n\n### Intent of the Code\n\nThe intent of the code is to provide a powerful and user-friendly tool that automates the tedious task of documenting Python code. By integrating directly with GitHub, it ensures that updates to documentation can be submitted for review quickly, facilitating better collaboration and code quality improvement.\n\n### Conclusion\n\nThis Python script serves as a comprehensive tool for automating the addition of docstrings to Python files while managing the complexities of GitHub PR creation. With its CLI interface, users have control over significant parameters that can affect the outcome of the script, such as file paths and GitHub configurations. Overall, it encapsulates best practices in software development, leveraging modern tools and APIs to improve efficiency in code documentation."
  },
  {
    "file": "docstring_ai/lib/github_utils.py",
    "description": "The provided Python file is a utility script designed to facilitate the addition of docstrings in Python files within a GitHub repository. It integrates various functionalities such as checking the status of the repository, creating branches, committing changes, and pushing them to a remote GitHub repository along with the option to create pull requests (PRs). Below is a comprehensive description of the script's functionalities, structure, purpose, and key components.\n\n### Overview\n\nThe script automates essential Git operations while ensuring that Python files have appropriate documentation. It does this while managing branches and connectively integrating with GitHub to streamline the process of submitting changes for review.\n\n### Main Functionalities\n\n1. **Branch Management**: Create and checkout branches for making changes to the codebase.\n2. **Git Operations**: Check for unstaged changes, stage files, commit changes, and push them to the GitHub repository.\n3. **Pull Request Creation**: Automate the creation of pull requests in GitHub once changes are committed and pushed.\n4. **GitHub Integration**: Use the GitHub API to interact with remote repositories, including fetching repository information and creating PRs.\n5. **Logging**: The script utilizes logging to provide feedback during operations, including errors, warnings, and informative messages to the user.\n\n### Functions Description\n\nThe script contains several functions, each with a specific purpose:\n\n1. **branch_exists(repo, branch_name)**\n   - **Purpose**: Checks if the specified branch already exists in the GitHub repository.\n   - **Args**: \n     - `repo`: The GitHub repository object.\n     - `branch_name`: The name of the branch to check.\n   - **Returns**: True if the branch exists; otherwise, False.\n\n2. **sanitize_branch_name(name: str) -> str**\n   - **Purpose**: Sanitizes the branch name, replacing invalid characters with underscores.\n   - **Args**: \n     - `name`: The original branch name.\n   - **Returns**: A sanitized branch name.\n\n3. **generate_unique_suffix() -> str**\n   - **Purpose**: Generates an 8-character unique identifier using UUID4.\n   - **Returns**: A string representing the unique suffix.\n\n4. **has_unstaged_changes(repo_path: str) -> bool**\n   - **Purpose**: Determines if there are unstaged changes in the Git repository.\n   - **Args**: \n     - `repo_path`: The path to the local Git repository.\n   - **Returns**: True if unstaged changes exist; otherwise, False.\n\n5. **get_staged_files(repo_path: str) -> List[str]**\n   - **Purpose**: Retrieves staged file paths that are ready to be committed.\n   - **Args**: \n     - `repo_path`: The path to the local Git repository.\n   - **Returns**: A list of staged file paths.\n\n6. **create_github_pr(...)**\n   - **Purpose**: Automates the creation of a GitHub pull request.\n   - **Args**:\n     - `repo_path`: Local path to the GitHub repository.\n     - `github_token`: GitHub token for authentication.\n     - `github_repo`: Repository identifier (owner/repo format).\n     - `branch_base_name`: Base name for creating a new branch.\n     - `pr_name`: Title for the pull request.\n     - `target_branch`: The branch where the PR should be merged.\n   - **Returns**: True if the PR was successfully created; otherwise, False.\n\n7. **get_python_files(repo_path: str) -> List[str]**\n   - **Purpose**: Retrieves a list of all Python files in the repository.\n   - **Args**: \n     - `repo_path`: The path to the local Git repository.\n   - **Returns**: A list of Python file paths.\n\n8. **create_pull_request_body(changed_files: List[str]) -> str**\n   - **Purpose**: Creates a formatted body for the pull request, listing the changed files.\n   - **Args**: \n     - `changed_files`: The list of changed file paths.\n   - **Returns**: A formatted PR body as a string.\n\n9. **commit_and_push_changes(...)**\n   - **Purpose**: Commits staged changes and pushes them to the remote branch.\n   - **Args**:\n     - `repo_path`: The local path to the GitHub repository.\n     - `branch_name`: The name of the branch for committing changes.\n     - `commit_message`: A message describing the commit.\n   - **Returns**: True if the commit and push were successful; otherwise, False.\n\n10. **log_git_status(repo_path: str) -> bool**\n    - **Purpose**: Logs the current status of the Git repository.\n    - **Args**: \n      - `repo_path`: The local path to the GitHub repository.\n    - **Returns**: True if status logging is successful; otherwise, False.\n\n11. **checkout_branch(repo_path: str, branch_name: str) -> bool**\n    - **Purpose**: Checks out the specified branch in the repository.\n    - **Args**: \n      - `repo_path`: The local path to the GitHub repository.\n      - `branch_name`: The name of the branch to checkout.\n    - **Returns**: True if the checkout is successful; otherwise, False.\n\n### Key Libraries Used\n\n- **os**: For file and directory handling operations.\n- **argparse**: To create a command-line interface for script execution.\n- **json**: For handling JSON data.\n- **chromadb**: For embedding and managing code contexts.\n- **tiktoken**: Typically for token management; not deeply covered in this script but relevant for NLP applications.\n- **datetime**: To handle date and time operations.\n- **uuid**: For generating unique identifiers.\n- **subprocess**: For executing shell commands like Git operations.\n- **logging**: To log events, warnings, and errors.\n- **re**: For regular expressions (used in sanitizing branch names).\n- **pathlib**: For filesystem path operations.\n\n### Purpose and Context\n\nThe main purpose of the script is to automate the process of documenting Python code through the addition of docstrings. It is particularly useful in collaborative software development scenarios where code readability and maintainability are paramount. By automating the creation of pull requests, developers can gather feedback on their changes more efficiently, thus enhancing collaboration.\n\n### Intent of the Code\n\nThe intent behind the code is to provide a seamless experience for developers looking to document their code and contribute to repositories on GitHub. By managing Git operations efficiently and integrating with GitHub to create PRs, it aims to streamline workflows for software teams and promote best practices in documentation.\n\n### Conclusion\n\nOverall, this Python script serves as a valuable tool for developers by combining Git operations with GitHub API interactions to facilitate automated documentation additions in Python projects. The structured approach employed in the script allows for easy modification and extension, making it a versatile addition to any developer's toolkit for code documentation."
  },
  {
    "file": "docstring_ai/lib/docstring_utils.py",
    "description": "The provided Python file is a module designed for extracting and handling docstrings from Python code. It uses the Abstract Syntax Tree (AST) to analyze code structures, retrieve docstrings, and gather information about classes and their parent classes. Furthermore, it integrates with the OpenAI API to potentially enhance the documentation by adding docstrings when needed. Below is a comprehensive breakdown of the module's functionality, purpose, structure, and components.\n\n### Overview\n\nThe module primarily features functions and a class focused on:\n1. **Extracting Descriptions from Docstrings**: Simple descriptions from various elements of the code (functions, classes, modules).\n2. **Extracting Class Docstrings**: Specific docstring extraction for classes.\n3. **Adding Docstrings**: Integrating with the OpenAI API to generate and insert docstrings into Python code.\n4. **Parsing Classes**: Identifying classes in a file along with their inheritance relationships.\n5. **Utilities for Logging and Error Handling**: Each function and method incorporates logging functionality to track errors and operational flow.\n\n### Main Functionalities\n\n1. **Docstring Extraction**: Retrieve and format descriptions from Python source code.\n2. **Class Parsing**: Identify classes and their parent classes within Python files.\n3. **Integration with OpenAI**: Utilize OpenAI\u2019s assistance for adding appropriate docstrings.\n4. **Logging**: Implement logging for monitoring and debugging the module's functions.\n\n### Function Descriptions\n\nThis module includes several key functions and a class with the following details:\n\n1. **extract_description_from_docstrings(code_with_docstrings: str) -> str**\n   - **Purpose**: Extracts the first line of docstrings from functions, classes, and modules.\n   - **Args**: Takes a string containing Python code with docstrings.\n   - **Returns**: A semicolon-separated string of extracted descriptions.\n   - **Raises**: Exception if an error occurs during parsing.\n\n2. **extract_class_docstring(code: str, class_name: str) -> str**\n   - **Purpose**: Extracts the docstring of a specified class from the code.\n   - **Args**: \n     - `code`: The Python code with class definitions.\n     - `class_name`: The name of the class.\n   - **Returns**: The class's docstring or an empty string if not found.\n   - **Raises**: Exception during extraction.\n\n3. **parse_classes(file_path: str) -> Dict[str, List[str]]**\n   - **Purpose**: Analyzes a Python file and returns a dictionary mapping class names to their parent classes.\n   - **Args**: \n     - `file_path`: Path to the Python file.\n   - **Returns**: A dictionary with class names as keys and their parent classes as values.\n   - **Raises**: Exception on file reading or parsing errors.\n\n### Class Definition\n\n#### DocstringExtractor\nThe `DocstringExtractor` class encapsulates functionality for extracting docstrings and imports from a specified Python file.\n\n1. **__init__(self, file_path: str)**\n   - **Purpose**: Initializes the extractor with the path to a Python file.\n   - **Args**: \n     - `file_path`: The path to the file to analyze.\n\n2. **read_file(self)**\n   - **Purpose**: Reads the content of the Python file and stores it.\n   - **Raises**: `FileNotFoundError` if the file is missing; `IOError` if there\u2019s a read error.\n\n3. **parse_ast(self)**\n   - **Purpose**: Parses the file content into an Abstract Syntax Tree (AST).\n   - **Raises**: `SyntaxError` if the code contains invalid syntax.\n\n4. **extract_docstrings(self)**\n   - **Purpose**: Extracts all docstrings from the AST and populates a dictionary with the results.\n   \n5. **list_imports_from_package(self, package: str)**\n   - **Purpose**: Lists all imported names from a specified package.\n   - **Args**: \n     - `package`: The package name to examine.\n   - **Returns**: A list of imported names.\n\n6. **compile(self)**\n   - **Purpose**: Compiles docstrings into a readable text format.\n   - **Returns**: A formatted string containing all extracted docstrings.\n\n7. **get_docstrings_dict(self)**\n   - **Purpose**: Returns the dictionary of extracted docstrings.\n   - **Returns**: The docstrings mapping.\n\n8. **process(self)**\n   - **Purpose**: High-level method to perform the docstring extraction operation.\n   - **Returns**: The dictionary of extracted docstrings.\n\n9. **process_imports(self, package: str)**\n   - **Purpose**: High-level method to list imports from a specified package.\n\n### Import Statements\n\n- **openai**: For interacting with the OpenAI API.\n- **time**: For handling time-related functions.\n- **ast**: To work with Python's Abstract Syntax Tree for analyzing code structure.\n- **logging**: For logging information and errors.\n- **typing**: To provide type hints throughout the module.\n- **docstring_ai**: Contains constants and utility functions related to the project.\n\n### Purpose and Context\n\nThe primary purpose of this module is to facilitate automated documentation of Python code, especially useful for large codebases where maintaining accurate and comprehensive docstrings is challenging. It is aimed at software developers and maintainers who need to ensure their code is adequately documented for both usability and maintainability.\n\n### Intent of the Code\n\nThe intent behind the code is to provide tools that streamline the process of documentation extraction and enhancement. By integrating with the OpenAI API, it aims to assist developers in generating meaningful documentation that can be easily reviewed and incorporated into existing codebases.\n\n### Conclusion\n\nThis Python module serves as a vital component for enhancing code documentation practices in Python projects. By automating the extraction of docstrings, identifying class definitions, and integrating with NLP technologies, it aims to improve code clarity and maintainability. The structured approach of the `DocstringExtractor` class, along with well-defined functions, allows for easy use and extension in wider documentation tools or workflows."
  },
  {
    "file": "docstring_ai/lib/prompt_utils.py",
    "description": "The provided Python file is a module designed to manage an AI assistant that specializes in adding docstrings to Python code. It uses OpenAI's API for creating and interacting with this assistant, along with functionalities for managing threads and constructing prompts based on contextual information stored in ChromaDB. Below is a comprehensive description of the module's functionalities, structure, purpose, and components.\n\n### Overview\n\nThe primary purpose of this module is to facilitate the process of documenting Python code by automating the addition of docstrings. The module integrates with OpenAI's language models for generating and managing these docstrings and provides methods to create and maintain context-aware interactions with the AI assistant.\n\n### Main Functionalities\n\n1. **Assistant Initialization**: Setup and management of the AI assistant instance using OpenAI's API.\n2. **Thread Management**: Creation and interaction with conversation threads to maintain context.\n3. **Docstring Generation**: Utilizing OpenAI's model to add comprehensive docstrings to Python classes and functions.\n4. **Contextual Prompt Construction**: Creating structured prompts based on relevant context gathered from ChromaDB.\n5. **Code Extraction**: Extracting code blocks from messages sent and received from the assistant.\n6. **Vector Store Manipulation**: Managing a vector store for efficient searches and retrievals in the assistant's operations.\n\n### Function and Class Descriptions\n\nThe module includes imports, a class definition, and multiple functions as described below:\n\n#### Class Definitions\n\n1. **PythonFile (BaseModel)**\n   - **Purpose**: A Pydantic model representing a Python file.\n   - **Attributes**:\n     - `new_file_content`: A string that contains the updated Python script with the added docstrings.\n\n#### Functions\n\n1. **initialize_assistant(api_key: str, assistant_name: str = \"DocstringAssistant\") -> str**\n   - **Purpose**: Initializes or retrieves an existing assistant by name.\n   - **Args**:\n     - `api_key`: The OpenAI API key for authentication.\n     - `assistant_name`: The name of the assistant to retrieve or create.\n   - **Returns**: The assistant ID or None if an error occurs.\n   - **Raises**: Exception if unable to retrieve or create the assistant.\n\n2. **update_assistant_tool_resources(api_key: str, assistant_id: str, file_ids: List[str]) -> None**\n   - **Purpose**: Updates the assistant's resources with uploaded file IDs.\n   - **Args**:\n     - `api_key`: The API key for OpenAI authentication.\n     - `assistant_id`: The ID of the assistant to update.\n     - `file_ids`: A list of file IDs to add to the resources.\n   - **Raises**: Exception if an error occurs during the update process.\n\n3. **create_thread(api_key: str, assistant_id: str, initial_messages: List[dict] = None) -> str**\n   - **Purpose**: Creates a new conversation thread for the assistant.\n   - **Args**:\n     - `api_key`: The API key for OpenAI authentication.\n     - `assistant_id`: The ID of the assistant.\n     - `initial_messages`: Initial messages to start the thread (optional).\n   - **Returns**: The ID of the created thread or None if an error occurs.\n   - **Raises**: Exception if thread creation fails.\n\n4. **construct_few_shot_prompt(collection: chromadb.Collection, classes: Dict[str, List[str]], max_tokens: int, context: str = None) -> str**\n   - **Purpose**: Constructs a few-shot prompt using relevant context summaries from ChromaDB.\n   - **Args**:\n     - `collection`: The ChromaDB collection for context retrieval.\n     - `classes`: A dictionary of class names and their parent classes.\n     - `max_tokens`: Maximum number of tokens for the prompt.\n     - `context`: Additional context to include in the prompt.\n   - **Returns**: The constructed few-shot prompt.\n   - **Raises**: Exception if an error occurs during prompt construction.\n\n5. **extract_code_from_message(message: str) -> str**\n   - **Purpose**: Extracts code blocks from the assistant's response message.\n   - **Args**: \n     - `message`: The message string from the assistant.\n   - **Returns**: The extracted code block.\n   - **Raises**: Exception if no code block is found.\n\n6. **send_message_to_assistant(assistant_id: str, thread_id: str, prompt: str, response_format: BaseModel = None, tools: List = [], tool_choice = \"auto\", functions: Dict[str, Callable] = {}) -> str**\n   - **Purpose**: Sends a prompt to the assistant and retrieves the response.\n   - **Args**:\n     - `assistant_id`: The ID of the assistant.\n     - `thread_id`: The ID of the thread for communication.\n     - `prompt`: The content sent to the assistant.\n     - `response_format`: Optional response format.\n     - `tools`: List of tools to be utilized by the assistant.\n     - `functions`: Dictionary of callable functions for tool responses.\n   - **Returns**: The assistant's response text or an error message.\n   - **Raises**: Various exceptions during interaction.\n\n7. **generate_file_description(assistant_id: str, thread_id: str, file_content: str) -> str**\n   - **Purpose**: Generates a detailed description of a Python file.\n   - **Args**:\n     - `assistant_id`: The ID of the assistant.\n     - `thread_id`: The ID of the thread.\n     - `file_content`: The content of the Python file.\n   - **Returns**: A description of the file.\n\n8. **create_file_with_docstring(assistant_id: str, thread_id: str, code: str, context: str, functions: Dict[str, Callable]) -> str**\n   - **Purpose**: Adds docstrings to the provided Python code using the assistant.\n   - **Args**:\n     - `assistant_id`: The ID of the assistant.\n     - `thread_id`: The ID of the thread.\n     - `code`: The Python code to process.\n     - `context`: Additional contextual information.\n     - `functions`: Callable functions for processing.\n   - **Returns**: The modified code with added docstrings.\n   - **Raises**: Exception if an error occurs.\n\n9. **create_vector_store(vector_store_name: str, file_ids: List[str]) -> str**\n   - **Purpose**: Creates a vector store associated with file IDs.\n   - **Args**:\n     - `vector_store_name`: The name for the vector store.\n     - `file_ids`: List of file IDs to associate with the vector store.\n   - **Returns**: The ID of the created vector store.\n\n10. **poll_run_completion(run_id: str, thread_id: str, functions: Dict[str, Callable]) -> bool**\n    - **Purpose**: Polls for the completion status of an AI run.\n    - **Args**:\n      - `run_id`: The ID of the monitoring run.\n      - `thread_id`: The thread ID for communication.\n    - **Returns**: True if the run completed successfully, False otherwise.\n\n11. **retrieve_last_assistant_message(thread_id: str) -> str**\n    - **Purpose**: Retrieves the last message from the specified thread.\n    - **Args**:\n      - `thread_id`: The thread ID to query.\n    - **Returns**: The content of the last message or None if no messages exist.\n\n### Import Statements\n\nThe module utilizes a variety of libraries:\n- **openai**: For API interactions with OpenAI's Assistant.\n- **chromadb**: For managing context and relevant data sources.\n- **pydantic**: For data validation using typed models.\n- **logging**: To manage and log application activities.\n- **json**: For handling JSON data operations.\n\n### Purpose and Context\n\nThe module is designed to facilitate the addition of docstrings in Python codebases by automating interactions with an AI assistant. It provides functionalities for initializing the assistant, managing contexts, and generating prompt structures to enhance the quality of documentation in software projects.\n\n### Intent of the Code\n\nThe code aims to create a seamless experience for developers looking to improve the documentation and maintainability of their Python code. By integrating the capabilities of OpenAI's models and contextual information from ChromaDB, the module ensures that developers can provide thorough and meaningful documentation with minimal manual effort.\n\n### Conclusion\n\nOverall, the provided Python module offers essential utilities for managing an AI assistant that helps in adding docstrings to Python code. By structuring its functionalities around thread management, prompt construction, and API interactions, it serves as a powerful tool for enhancing code documentation practices in software development. Its design reflects an intention to streamline workflows while leveraging NLP technologies effectively."
  },
  {
    "file": "docstring_ai/lib/process.py",
    "description": "The provided Python file is a module designed to automate the process of adding docstrings to Python code files using an OpenAI assistant. It integrates with ChromaDB for context embedding, manages interactions with OpenAI's API, and interfaces with GitHub for pull request creation. Below is a comprehensive description of the module's functionality, purpose, classes, and methods.\n\n### Overview\n\nThe module provides a systematic approach to enhancing Python code by automatically generating docstrings based on the code's context. It processes files, updates the documentation, stores relevant information in ChromaDB, and facilitates version control through GitHub.\n\n### Main Functionalities\n\n1. **Initialize and Manage an AI Assistant**: Set up an OpenAI assistant specifically for adding docstrings to Python code.\n2. **Process Python Files**: Read Python files, generate descriptions, and add docstrings based on class definitions and other relevant contexts.\n3. **Embed Context**: Store file information and context in ChromaDB for efficient retrieval.\n4. **Generate Pull Requests**: Optionally create pull requests on GitHub to submit the newly updated code for review.\n5. **Logging and Error Management**: Implement logging to track the execution flow and handle errors effectively.\n\n### Functions Description\n\nThe module comprises several functions, each designed with specific tasks in mind:\n\n1. **initialize_and_create_assistant(api_key: str)**:\n   - **Purpose**: Initializes an OpenAI assistant and creates a thread for interaction.\n   - **Args**: \n     - `api_key`: The OpenAI API key for authentication.\n   - **Returns**: A tuple containing the Assistant ID and Thread ID.\n   - **Raises**: Exception if initialization fails.\n\n2. **process_file_descriptions(files_to_process: List[str], output_dir: Path, assistant_id: str, thread_id: str, context_summary: List[Dict], collection, api_key: str, repo_path: str)**:\n   - **Purpose**: Generate descriptions for the specified files, embed them into ChromaDB, and upload to OpenAI.\n   - **Args**: \n     - `files_to_process`: List of file paths.\n     - `output_dir`: Directory to store description files.\n     - Other parameters related to assistant context and resources.\n   - **Returns**: List of uploaded description file IDs.\n\n3. **process_files_and_create_prs(repo_path: str, api_key: str, create_pr: bool, github_token: str, github_repo: str, branch_name: str, pr_name: str, pr_depth: int, manual: bool, target_branch: str)**:\n   - **Purpose**: Processes the Python files, adds docstrings, and manages GitHub PR creation.\n   - **Args**: Various parameters relevant to the repository, OpenAI API, and GitHub configuration.\n   - **Returns**: None.\n\n4. **process_single_file(python_file_path: str, repo_path: str, assistant_id: str, thread_id: str, collection, context_summary: list, cache: dict, manual: bool)**:\n   - **Purpose**: Handles the processing of a single Python file, including reading content, generating docstrings, and handling caching.\n   - **Args**: Parameters containing paths, IDs, and settings for processing.\n   - **Returns**: None.\n\n5. **approve_and_save_file(new_file_content: str, original_code: str, python_file_path: str, repo_path: str, manual: bool, context_summary: list, cache: dict, collection, assistant_id: str, thread_id: str)**:\n   - **Purpose**: Saves the updated file content and manages manual approval, caching, and context updates.\n   - **Args**: Parameters containing file contents and processing context.\n   - **Returns**: Boolean indicating success or failure.\n\n6. **filter_files_by_hash(file_paths: List[str], repo_path: str, cache: Dict[str, str]) -> List[str)**:\n   - **Purpose**: Filters out files that have not changed based on their SHA-256 hash.\n   - **Args**: Lists of file paths, repository paths, and a cache of previously processed files.\n   - **Returns**: List of files that require processing.\n\n7. **upload_files_to_openai(file_paths: List[str]) -> List[str)**:\n   - **Purpose**: Uploads files to OpenAI and returns their IDs.\n   - **Args**: List of file paths to upload.\n   - **Returns**: List of uploaded file IDs.\n\n### Important Classes\n\n1. **PythonFile (BaseModel)**:\n   - **Attributes**: \n     - `new_file_content`: String that holds the new content of the Python file after docstring addition.\n\n### Key Imports\n\n- **openai**: Interface with OpenAI's API for AI interactions.\n- **logging**: For tracking the application flow and errors.\n- **tqdm**: For progress bar visualization during file processing.\n- **ast**: Abstract Syntax Tree for analyzing Python code.\n- **pydantic**: For structured data validation using `BaseModel`.\n- **ChromaDB**: For context management and embedding information.\n  \n### Purpose and Context\n\nThe module is aimed at developers and maintainers who wish to improve the quality of their Python code by ensuring that functions, classes, and modules are properly documented with docstrings. With the integration of AI assistance, it provides an automated solution to enhance documentation practices in Python programming.\n\n### Intent of the Code\n\nThe module's intent is to streamline the process of adding docstrings to Python files by leveraging AI, thereby enhancing the usability and readability of the code. It reduces the manual effort required for documentation while maintaining an organized approach to managing changes in a version-controlled system.\n\n### Conclusion\n\nIn summary, this Python module efficiently automates the tasks of processing Python files for documentation enhancement using an AI assistant. By integrating various tools, including OpenAI and ChromaDB, it manages to create a robust environment for maintaining quality documentation in software development projects. This organized structure along with comprehensive logging and error handling make it a reliable component for any Python developer looking to improve their code documentation practices."
  }
]