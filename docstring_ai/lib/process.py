# Docstring generated by docstring-ai : http://github.com/ph-ausseil/docstring-ai
"""
This module processes Python files to add docstrings using OpenAI's Assistant,
embeds the files in ChromaDB, and integrates with GitHub for pull request creation.

Functions:
- process_files_and_create_prs: Processes Python files, adds docstrings, and creates pull requests.
"""

import json
import os
import logging
from pathlib import Path
from typing import List, Dict
from functools import partial, wraps
from datetime import datetime

import openai
import tiktoken
from tqdm import tqdm

from docstring_ai.lib.utils import (
    ensure_docstring_header,
    check_git_repo,
    has_uncommitted_changes,
    file_has_uncommitted_changes,
    load_cache,
    save_cache,
    get_python_files,
    sort_files_by_size,
    prompt_user_confirmation,
    show_diff,
    compute_sha256,
    traverse_repo,
    create_backup
)
from docstring_ai.lib.prompt_utils import (
    initialize_assistant,
    update_assistant_tool_resources,
    create_thread,
    construct_few_shot_prompt,
    create_file_with_docstring,
    generate_file_description
)
from docstring_ai.lib.chroma_utils import (
    initialize_chroma,
    get_or_create_collection,
    embed_and_store_files,
    store_class_summary
)
from docstring_ai.lib.docstring_utils import (
    DocstringExtractor,
)
from docstring_ai.lib.github_utils import create_github_pr
from docstring_ai import (
    MAX_TOKENS, 
    CHROMA_COLLECTION_NAME, 
    CACHE_FILE_NAME, 
    DATA_PATH, 
    CONTEXT_SUMMARY_PATH
)


def process_files_and_create_prs(
    repo_path: str, 
    api_key: str, 
    create_pr: bool, 
    github_token: str, 
    github_repo: str, 
    branch_name: str, 
    pr_name: str, 
    pr_depth: int, 
    manual: bool
) -> None:
    """
    Processes Python files in the specified repository, adds docstrings using OpenAI's Assistant,
    and creates pull requests on GitHub if specified.

    This function orchestrates the entire workflow including:
    1. Verifying the presence of a Git repository and checking for uncommitted changes.
    2. Initializing ChromaDB for context-aware file embedding and retrieval.
    3. Loading a cache file to track previously processed files.
    4. Retrieving, sorting, and processing files for docstring addition using the OpenAI Assistant.
    5. Potentially creating pull requests based on processed files.

    Args:
        repo_path (str): The path of the Git repository to process.
        api_key (str): The OpenAI API key for making requests.
        create_pr (bool): Flag indicating if pull requests should be created.
        github_token (str): The GitHub token for authentication.
        github_repo (str): The repository where pull requests will be made.
        branch_name (str): The name of the branch for the pull request.
        pr_name (str): The name of the pull request to create.
        pr_depth (int): The maximum depth to categorize folders for PR creation.
        manual (bool): Flag indicating if manual approval is required for changes.

    Returns:
        None: This function performs its operations and does not return a value.

    Raises:
        Exception: Various exceptions may occur during file processing, API calls,
                    or Git operations, which are logged accordingly.
    """
    ###
    ### INITIATE
    ###
    openai.api_key = api_key

    # Check if Git is present
    git_present = check_git_repo(repo_path)

    # Initialize ChromaDB
    logging.info("\nInitializing ChromaDB...")
    chroma_client = initialize_chroma()
    collection = get_or_create_collection(chroma_client, CHROMA_COLLECTION_NAME)

    # Load cache
    cache_path = os.path.join(repo_path, CACHE_FILE_NAME)
    cache = load_cache(cache_path)

    # Step 1: Retrieve all Python files
    python_files = get_python_files(repo_path)
    logging.info(f"Found {len(python_files)} Python files to process.")

    if not python_files:
        logging.info("No Python files found. Exiting.")
        return

    # Step 2: Sort files by size (ascending)
    python_files_sorted = sort_files_by_size(python_files)

    # Step 3: Compute SHA-256 hashes and filter out unchanged files
    logging.info("\nChecking file hashes...")
    files_to_process = filter_files_by_hash(python_files_sorted, repo_path, cache)
    logging.info(f"\n{len(files_to_process)} files to process after cache check.")

    if not files_to_process:
        logging.info("No files need processing. Exiting.")
        return

    # Step 4: Traverse repository and categorize folders based on pr_depth
    logging.info("\nTraversing repository to categorize folders based on pr_depth...")
    folder_dict = traverse_repo(repo_path, pr_depth)
    logging.info(f"Found {len(folder_dict)} depth levels up to {pr_depth}.")

    ###
    ### EMBED
    ###

    # Load Context Summary
    context_summary_full_path = os.path.join(repo_path, CONTEXT_SUMMARY_PATH)
    context_summary = []
    if os.path.exists(context_summary_full_path):
        try:
            with open(context_summary_full_path, 'r', encoding='utf-8') as f:
                context_summary = json.load(f)
            logging.info(f"Loaded context summary with {len(context_summary)} entries.")
        except Exception as e:
            logging.error(f"Error loading context summary: {e}")

    # Step 5: Embed and store files in ChromaDB
    logging.info("\nEmbedding and storing Python files in ChromaDB...")
    embed_and_store_files(collection=collection, python_files=files_to_process, tags={"file_type" : "script"})

    # Step 6: Upload files to OpenAI and update Assistant's tool resources
    logging.info("\nUploading files to OpenAI...")
    python_file_ids = upload_files_to_openai(files_to_process)

    if not python_file_ids:
        logging.error("No files were successfully uploaded to OpenAI. Exiting.")
        return

    ###
    ### START ASSISTANT 
    ###
    # Step 7: Initialize Assistant
    logging.info("\nInitializing Assistant...")
    assistant_id = initialize_assistant(api_key)
    if not assistant_id:
        logging.error("Assistant initialization failed. Exiting.")
        return

    # Update Assistant's tool resources with OpenAI file IDs
    update_assistant_tool_resources(
        api_key=api_key,
        assistant_id=assistant_id,
        file_ids=python_file_ids
    )

    # Step 8: Create a Thread
    logging.info("\nCreating a new Thread...")
    thread_id = create_thread(
        api_key=api_key, 
        assistant_id=assistant_id
    )
    if not thread_id:
        logging.error("Thread creation failed. Exiting.")
        return

    # Ensure the './data/' directory exists
    output_dir = DATA_PATH
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Step 9: Create Missing Context 
    file_descriptions_list = []
    for file in files_to_process: 
        if not any(entry["file"] == file for entry in context_summary): 
            logging.info(f"Generating detailed description for {file}...")
            with open(file, 'r', encoding='utf-8') as f:
                file_description = generate_file_description(
                    assistant_id=assistant_id,
                    thread_id=thread_id, 
                    file_content=f.read()
                )

            file_path = Path(output_dir) / Path(file).with_suffix('.txt')
            file_path.parent.mkdir(parents=True, exist_ok=True) 
            # Create a file with descriptions
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(file_description)
            file_descriptions_list.append(file_path)

            context_summary.append({
                "file": file,
                "description": file_description
            })
            # Append to context_summary for future use

    # Step 10: Embed and store context descriptions in ChromaDB
    logging.info("\nEmbedding and storing Python file descriptions in ChromaDB...")
    embed_and_store_files(
        collection=collection, 
        python_files=file_descriptions_list,
        tags={"file_type": "description"})

    # Step 11: Upload context descriptions to OpenAI and update Assistant's tool resources
    logging.info("\nUploading file descriptions to OpenAI...")
    description_file_ids = upload_files_to_openai(file_paths=[str(fp) for fp in file_descriptions_list])

    update_assistant_tool_resources(
        api_key=api_key,
        assistant_id=assistant_id,
        file_ids= python_file_ids + description_file_ids)

    # Step 12: Process Each Python File for Docstrings
    logging.info("\nProcessing Python files to add docstrings...")
    with tqdm(total=len(files_to_process), desc="Adding docstrings", unit="file", dynamic_ncols=True) as pbar:
        for python_file_path in files_to_process:
            process_single_file(
                python_file_path=python_file_path,
                repo_path=repo_path,
                assistant_id=assistant_id,
                thread_id=thread_id,
                collection=collection,
                context_summary=context_summary,
                cache=cache,
                manual=manual
            )
            pbar.update(1)

    # Step 13: Save Context Summary
    try:
        with open(context_summary_full_path, "w", encoding='utf-8') as f:
            json.dump(context_summary, f, indent=2)
        logging.info(f"\nDocstring generation completed. Context summary saved to '{context_summary_full_path}'.")
    except Exception as e:
        logging.error(f"Error saving context summary: {e}")

    # Step 14: Save Cache
    save_cache(cache_path, cache)

    # Step 15: Create Pull Requests Based on pr_depth
    if create_pr and git_present and github_token and github_repo:
        if manual:
            # Show summary of PRs to be created and ask for confirmation
            print("\nPull Requests to be created for the following folders:")
            for depth, folders in folder_dict.items():
                for folder in folders:
                    print(f"- Depth {depth}: {folder}")
            if not prompt_user_confirmation("Do you want to proceed with creating these Pull Requests?"):
                logging.info("Pull Request creation aborted by the user.")
                return

        logging.info("\nCreating Pull Requests based on pr_depth...")
        for depth, folders in folder_dict.items():
            for folder in folders:
                # Collect all Python files in the folder
                pr_files = get_python_files(folder)
                if not pr_files:
                    continue  # Skip folders with no Python files

                # Generate a unique branch name for the folder
                folder_rel_path = os.path.relpath(folder, repo_path).replace(os.sep, "_")
                folder_branch_name = f"feature/docstrings-folder-{folder_rel_path}-{datetime.now().strftime('%Y%m%d%H%M%S')}"

                # Generate PR name
                folder_pr_name = f"-- Add docstrings for folder `{folder_rel_path}`" if not pr_name else pr_name

                # Create GitHub PR

                create_github_pr(
                    repo_path=repo_path, 
                    github_token=github_token, 
                    github_repo=github_repo, 
                    branch_base_name=folder_branch_name, 
                    pr_name=folder_pr_name
                )


def process_single_file(
    python_file_path: str,
    repo_path: str,
    assistant_id: str,
    thread_id: str,
    collection,
    context_summary: list,
    cache: dict,
    manual: bool
) -> None:
    """
    Processes a single Python file: adds docstrings, updates context, and handles caching.

    **Steps:**

    1. **File Reading and Initial Setup**
       - Compute the file's relative path within the repository.
       - Read the file content into memory.
       - Log errors and skip processing if the file cannot be read.

    2. **Class Parsing and Context Retrieval**
       - Parse the file for class definitions and dependencies.
       - If classes are found, retrieve relevant context summaries from ChromaDB to support docstring generation.

    3. **Few-Shot Prompt Construction**
       - Construct a few-shot prompt using the retrieved context and examples from the ChromaDB collection.

    4. **File Description Generation**
       - Check if the file's description is already cached.
       - If not cached, generate a detailed file description using the OpenAI Assistant and append it to the context summary.

    5. **Docstring Generation**
       - Use the OpenAI Assistant to generate docstrings for the code, applying the few-shot prompt and contextual information.

    6. **Manual Validation (Optional)**
       - If `manual` is set to True, display the differences between the original and modified code for user approval before saving changes.

    7. **File Update**
       - Create a backup of the original file if Git is not present or if the file has uncommitted changes.
       - Overwrite the file with the modified code containing the new docstrings.

    8. **Context Summary and Cache Updates**
       - Generate an updated file description after docstring addition.
       - Update the cache with the new file hash to avoid reprocessing unchanged files.
       - Update the context summary with the new description.

    9. **Class Summaries**
       - Extract and store summaries for any modified classes in the file.

    Args:
        file_path (str): The path to the Python file.
        repo_path (str): The repository path.
        assistant_id (str): The OpenAI assistant ID.
        thread_id (str): The thread ID for OpenAI interactions.
        collection: The ChromaDB collection.
        context_summary (list): The context summary list.
        cache (dict): The cache dictionary.
        manual (bool): Flag indicating if manual approval is required.

    Returns:
        None
    """
    try:
        relative_path = os.path.relpath(python_file_path, repo_path)
        with open(python_file_path, 'r', encoding='utf-8') as f:
            original_code = f.read()
    except Exception as e:
        logging.error(f"Error reading file {python_file_path}: {e}")
        return

    # Check if file is cached and has existing description
    cached_entry = next((item for item in context_summary if str(Path(item["file"])) == str(Path(relative_path))), None)
    if cached_entry:
        file_description = cached_entry.get("description", "")
        logging.info(f"Using cached description for {python_file_path}.")
    else: 
        logging.error("No file description: Cached or Created in `process_files_and_create_prs` ")
        file_description = ""  # Initialize to empty string or handle accordingly

    extractor = DocstringExtractor(file_path=python_file_path)
    extractor.process()
    classes = extractor.process_imports(package='docstring_ai.lib')    

    # Construct few-shot prompt
    few_shot_prompt = construct_few_shot_prompt(
        collection=collection, 
        classes=classes, 
        max_tokens=MAX_TOKENS - len(original_code),
        context=file_description
    )

    # Add docstrings using Assistant's API
    logging.info(f"Generating new docstrings for: {python_file_path}")

    # Create a partial function for approval and saving
    patched_approve_and_save_file = partial(
        approve_and_save_file,
        original_code=original_code,
        python_file_path=python_file_path,
        repo_path=repo_path,
        manual=manual,
        context_summary=context_summary,
        cache=cache,
        collection=collection,
        assistant_id=assistant_id,
        thread_id=thread_id,
    )

    # Generate and apply docstrings
    result = create_file_with_docstring(
        assistant_id=assistant_id,
        thread_id=thread_id,
        code=original_code,
        context=few_shot_prompt,
        functions={"write_file_with_new_docstring": patched_approve_and_save_file}
    )

    if result:
        logging.info(f"Docstrings successfully added and saved for {python_file_path}.")
    else:
        logging.warning(f"Docstrings not added for {python_file_path}.")


def approve_and_save_file(
    new_file_content: str,
    original_code: str,
    python_file_path: str,
    repo_path: str,
    manual: bool,
    context_summary: list,
    cache: dict,
    collection,
    assistant_id: str,
    thread_id: str,
) -> bool:
    """
    Approves and saves the modified code, handles manual validation, updates context, and cache.

    Args:
        new_file_content (str): The code after adding docstrings.
        original_code (str): The original code before modification.
        file_path (str): The path to the Python file.
        repo_path (str): The repository path.
        manual (bool): Flag indicating if manual approval is required.
        context_summary (list): The context summary list.
        cache (dict): The cache dictionary.
        collection: The ChromaDB collection.
        assistant_id (str): The OpenAI assistant ID.
        thread_id (str): The thread ID for OpenAI interactions.

    Returns:
        bool: True if the file was successfully updated and saved, False otherwise.
    """
    # Check if there's any change in the file content
    if not new_file_content:
        logging.info(f"No changes made to {python_file_path}.")
        # Update cache even if no changes to prevent reprocessing unchanged files
        relative_path = os.path.relpath(python_file_path, repo_path)
        try:
            current_hash = compute_sha256(python_file_path)
            cache[relative_path] = current_hash
            logging.info(f"Cache updated for unchanged file: {python_file_path}")
        except Exception as e:
            logging.error(f"Error computing hash for {python_file_path}: {e}")
        return False  # Indicate that no changes were made

    if new_file_content == original_code:
        logging.info(f"No changes detected for {python_file_path}.")
        return False  # Indicate that no changes were made

    # Ensure the header is added if not present
    try:
        new_file_content = ensure_docstring_header(new_file_content)
    except Exception as e:
        logging.error(f"Error ensuring docstring header for {python_file_path}: {e}")
        return False

    # Handle manual approval if required
    if manual:
        try:
            diff = show_diff(original_code, new_file_content)
            print(f"\n--- Diff for {python_file_path} ---\n{diff}\n--- End of Diff ---\n")
            if not prompt_user_confirmation(f"Do you approve changes for {python_file_path}?"):
                logging.info(f"Changes for {python_file_path} were not approved by the user.")
                return False  # Indicate that changes were not approved
        except Exception as e:
            logging.error(f"Error during manual approval for {python_file_path}: {e}")
            return False

    try:
        # Backup original file only if needed
        if not check_git_repo(repo_path):
            # No Git: Always create a backup
            create_backup(python_file_path)
            logging.info(f"File Backup created for {python_file_path} (no Git).")
        elif file_has_uncommitted_changes(repo_path, python_file_path):
            # Git is present: Backup if the file has uncommitted changes
            create_backup(python_file_path)
            logging.info(f"Backup created for {python_file_path} (uncommitted changes).")

        # Update the file with modified code
        with open(python_file_path, "w", encoding="utf-8") as f:
            f.write(new_file_content)
        logging.info(f"Updated docstrings in {python_file_path}")

        # Generate updated file description after adding docstrings
        try:
            updated_file_description = generate_file_description(
                assistant_id=assistant_id,
                thread_id=thread_id,
                file_content=new_file_content,
            )
            logging.info(f"Updated description for {python_file_path}")
        except Exception as e:
            logging.error(f"Error generating updated description for {python_file_path}: {e}")
            return False

        # Update context_summary with the updated description
        relative_path = os.path.relpath(python_file_path, repo_path)
        try:
            cached_entry = next(
                (
                    item
                    for item in context_summary
                    if str(Path(item["file"])) == str(Path(relative_path))
                ),
                None,
            )
            if cached_entry:
                # Update existing entry
                cached_entry["description"] = updated_file_description
                logging.info(f"Updated cached description for {python_file_path}.")
            else:
                # Append new entry
                context_summary.append(
                    {
                        "file": relative_path,
                        "description": updated_file_description,
                    }
                )
                logging.info(f"Added new description for {python_file_path} to context summary.")
        except Exception as e:
            logging.error(f"Error updating context summary for {python_file_path}: {e}")
            return False

        # Update cache with new hash
        try:
            new_hash = compute_sha256(python_file_path)
            cache[relative_path] = new_hash
            logging.info(f"Updated cache for file: {python_file_path}")
        except Exception as e:
            logging.error(f"Error computing new hash for {python_file_path}: {e}")
            return False

        # Process class summaries
        try:
            extractor = DocstringExtractor(file_path=python_file_path)
            extractor.process()
            modified_classes = extractor.process_imports(package="docstring_ai.lib")

            for class_name in modified_classes:
                # Extract the docstring for each class
                class_docstring = extractor.get_class_docstring(class_name)
                if class_docstring:
                    summary = extractor.compile()  # Assuming compile returns the first line as summary
                    store_class_summary(
                        collection, relative_path, class_name, summary
                    )
                    logging.info(f"Stored summary for class {class_name} in {python_file_path}.")
        except Exception as e:
            logging.warning(f"Error refreshing internal vector for file {python_file_path}: {e}")

        return True  # Indicate success

    except Exception as e:
        logging.error(f"Error updating file {python_file_path}: {e}")
        return False  # Indicate failure


def filter_files_by_hash(file_paths: List[str], repo_path: str, cache: Dict[str, str]) -> List[str]:
    """
    Filters a single file based on SHA-256 hash and cache.

    Args:
        file_path (str): Path to the file.
        repo_path (str): Path to the repository.
        cache (dict): Dictionary storing file hashes.

    Returns:
        str: File path if it needs processing, otherwise None.
    """
    changed_files = []
    logging.info("Début de la vérification des hashs de fichiers...")
    
    with tqdm(total=len(file_paths), desc="Vérification des hashs de fichiers", unit="fichier") as pbar:
        for file_path in file_paths:
            try:
                current_hash = compute_sha256(file_path)
                relative_path = os.path.relpath(file_path, repo_path)
                cached_hash = cache.get(relative_path)
                
                if current_hash != cached_hash:
                    changed_files.append(file_path)
            except Exception as e:
                logging.error(f"Erreur lors de la vérification de {file_path} : {e}")
            finally:
                pbar.update(1)
    
    logging.info(f"Vérification terminée. {len(changed_files)} fichiers nécessitent un traitement.")
    return changed_files


def upload_files_to_openai(file_paths: List[str]) -> List[str]:
    """
    Uploads a single file to OpenAI and returns the file ID.

    Args:
        file_path (str): Path to the file.

    Returns:
        str: File ID from OpenAI or None on failure.
    """
    file_ids = []
    logging.info("Début du téléchargement des fichiers sur OpenAI...")
    with tqdm(total=len(file_paths), desc="Téléchargement des fichiers sur OpenAI", unit="fichier") as pbar:
        for file_path in file_paths:
            try:
                with open(file_path, "rb") as f:
                    response = openai.files.create(
                        file=f,
                        purpose="assistants"
                    )
                file_ids.append(response.id)
                logging.info(f"Fichier téléchargé : {file_path} avec l'ID : {response.id}")
            except Exception as e:
                logging.error(f"Erreur lors du téléchargement de {file_path} : {e}")
            finally:
                pbar.update(1)
    logging.info(f"Téléchargement terminé. {len(file_ids)} fichiers téléchargés avec succès.")
    return file_ids

